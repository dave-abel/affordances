\documentclass[]{article} 
\usepackage{proceed2e}

\usepackage[numbers,sort]{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{algorithm}
\usepackage{algpseudocode}

\title{Planning with Affordances}

\begin{document}
\author{}
\maketitle

\begin{abstract}
Current methods for exactly solving decision-making under uncertainty
require exhaustive enumeration of all possible states and actions,
leading to exponential run times, leading to the well-known ``curse of
dimensionality.''  Approaches to address this problem by providing the
system with formally encoded knowledge such as options or
macro-actions still fail to prevent the system from considering many
actions which seem obviously irrelevant for a human partner.  To
address this issue, we introduce a novel approach to representing
knowledge about how to plan in terms of {\em
  affordances}~\citep{gibson77}.  Our affordance formalism and
associated planning framework allows an agent to efficiently prune its
action space based on domain knowledge.  This pruning significantly
reduces the number of state/action pairs the agent needs to evaluate
in order to act optimally.  We demonstrate our approach in the
Minecraft domain on several planning and building tasks, showing a
significant increase in speed and reduction in state-space exploration
compared to subgoal planning, options, and macro-actions.
\end{abstract}

\section{INTRODUCTION}

As robots move out of the lab and into the real world, planning
algorithms need to be able to scale to domains of increased noise,
size, and complexity.  A classic formalization of this issue is the
sequential decision making problem, where increases in problem size
and complexity directly correspond to an explosion in the state-action
space. Current approaches to solving sequential decision making
problems cannot tackle these problems as the state-action space
becomes large~\citep{grounds05}.

There is a strong need for a generalizable form of knowledge that,
when coupled with a planner, is capable of solving problems in these
massive domains. Humans provide an excellent existence proof for such
planning, as we are capable of searching over an immense number of
possible actions when presented with a goal.  One approach to
explaining how humans solve this planning problem is by focusing on
problem-specific aspects of the environment which focus the search
toward the most relevant and useful parts of the state-action space.
Formally, \citet{gibson77} proposed to define this intuition as an
{\em affordance}, ``what [the environment] offers [an] animal, what
[the environment] provides or furnishes, either for good or ill.''
Additionally, roboticists have recently become interested in
leveraging affordances for perception and prediction of
human actions~\citep{koppula13a, koppula13b}.

\begin{figure}
\centering
\includegraphics[scale = 0.21]{figures/bridgeworld_vi_vs_aff.png}
\caption{In the above Minecraft planning problem \texttt{BRIDGEWORLD},
the agent must place a block in the trench in order to reach the goal 
(the trench is too wide to jump over). This problem was only solvable 
by the Affordance planner.\label{fig:bridgeworld_aff} (see Fig. \ref{fig:all_actions}
for a description of the action-symbols)}
\end{figure}

In this paper we will formalize
the notion of an affordance as a piece of planning knowledge provided
to an agent operating in a Markov Decision Process
(MDP)~\citep{kaelbling99}.  We demonstrate that, like an option or
macro-action, an affordance provides additional information to the
agent, enabling more transferable and efficient planning.  However, unlike previous
approaches, an affordance enables more significant speedups by
reducing the size and branching-factor of the search space, enabling
an agent to focus its search on the most relevant part of the problem
at hand.  This approach means that a {\em single} set of affordances
provides general domain knowledge, becoming relevant just when the
agent reasons that it needs to pursue a particular goal.  Furthermore,
Affordances are not specific to a particular state-space nor problem-type, and thus, provide
the agent with transferrable knowledge that is effective in a wide variety of
domains and problems, unlike other approaches.


\begin{figure}
\centering
\includegraphics[scale = 0.15]{figures/all_actions.png}
\caption{The set of all actions in the Minecraft domain \label{fig:all_actions}}
\end{figure}

\section{BACKGROUND}

\subsection{OO-MDP}

The Object Oriented Markov Decision Process 
(OO-MDP) ~\citep{diuk08} is an extension of the classic
Markov Decision Process (MDP), a fundamental building 
block of Reinforcement Learning (RL). RL is an algorithmic 
approach to sequential decision making problems, or more simply: planning.

A finite MDP is a five-tuple: $\langle \mathcal{S}, \mathcal{A}, 
\mathcal{T}, \mathcal{R}, \gamma \rangle$, where $\mathcal{S}$
is a state-space, $\mathcal{A}$ is the agent's set of actions, 
$\mathcal{T}$ denotes $\mathcal{T}(s' \mid s,a)$, the transition
probability of an agent applying action $a \in \mathcal{A}$ in 
state $s \in \mathcal{S}$ and arriving in $s' \in \mathcal{S}$, 
and $\mathcal{R}(s,a)$ denotes the reward at $s$ when action
$a$ is applied, and $\gamma$ is a discount factor.

The OO-MDP changes the representation of the state space 
$S$ through the introduction of object classes, each of which 
has a set of attributes. The state space is represented as a 
collection of objects, which are instances of the aforementioned 
classes. Additionally, upon instantiation the attributes of the 
object's class are given a state (an assignment of values). 
Finally, the underlying MDP is the union of all the states of its 
objects \citep{diuk08}.

Our motivation for using an OO-MDP instead of an MDP
lies in the ability to formulate predicates over classes of 
objects. As we will see in section 3, this helps us form 
preconditions and goals that generalize beyond a particular
instance of a state space.

As with a classical finite MDP, planning with an OO-MDP 
involves running Value Iteration to determine a policy. 
Recall that value iteration involves propagating reward 
throughout the state space until propagation has converged, 
at which point, a policy may be deduced. Reward propagation
occurs as in a Bellman update:

\begin{equation}
U_{i+1}(s) \leftarrow \mathcal{R}(s) + \gamma \max_{a \in \mathcal{A}(s)} \sum_{s'} \text{Pr}(s' \mid s, a)U_i(s')
\end{equation}

Where $U_i(s)$ is the {\it utility} of state $s$ at iteration $i$, 
representing the expected reward of being in that state. See Algorithm \ref{alg:vi} for the full pseudocode of the algorithm ~\citep{russellnorvigAI}.

% VI pseudocode from Norvig/Russell
\begin{algorithm}
  \caption{Value-Iteration($\mathcal{A}$, $\mathcal{R}$, $\mathcal{S}$, $\epsilon$, $\gamma$)}
  \begin{algorithmic}[1]
    \While {$\delta < \epsilon \frac{(1-\gamma)}{\gamma}$}
    \State $U \gets U';\delta \gets 0$
    \For {each state $s \in \mathcal{S}$}
    \State $U'[s] \leftarrow \mathcal{R}(s) + \gamma \max_{a \in \mathcal{A}(s)} \sum_{s'} \text{Pr}(s'\mid s,a) U[s']$
    \If {$|U'[s] - U[s]| > \delta$}
    	\State $\delta \gets |U'[s] - U[s]|$ 
    \EndIf
    \EndFor
    \EndWhile
    \Return U;
  \end{algorithmic}
  \label{alg:vi}
\end{algorithm}


In practice, basic Value Iteration scales very poorly, either as the state 
space grows, or the action set grows. This is because the state-action 
space, depending on the domain, grows exponentially w.r.t the number 
of ways that the agent can change the environment. This is ameliorated 
slightly by introducing the OO-MDP, but it still fails in just about all of 
the planning scenarios we introduce here.

The reason for this failure is that in classical Value Iteration, the agent tries to 
explore all states that result from applying every action in every state - this is downright silly, 
as in \texttt{BRIDGEWORLD}, the agent will inevitably end up in the corner of the 
room placing and destroying blocks. This is an especially bad tactic in the Minecraft 
domain, as block placement results in a combinatoric explosion of the state space 
(see Equation \ref{eq:mc_explode}). Thus, in these scenarios, our Affordance planner 
has a substantial advantage on classic Value Iteration. These weaknesses are well 
known ~\citep{grounds05}, and have resulted in attempts to make planning more 
practical in domains of a larger scale.

% More VI math. Maybe pseudocode.

\subsection{SUBGOALS}
% % Intuition
Subgoal planning leverages the intuition that certain goals in planning domains 
may only be brought about if certain preconditions are first satisfied. For instance, 
in the Minecraft domain, one must be in possession of grain in order to bake bread. 
In Branavan et. al~\citep{branavan12a}, they explore learning subgoals from the 
Minecraft wiki and applying them in order to plan through a variety of problems in Minecraft.

% Formalism
Formally, in subgoal planning, the agent is set of subgoals, where each subgoal is a pair of predicates:

\[ 
SG = \langle x_k, x_l \rangle
\]

where $x_l$ is the effect of some action sequence performed on 
a state in which $x_k$ is true. Thus, subgoal planning requires 
that we perform high-level planning in subgoal space, and low-level 
planning to get from subgoal to subgoal.

% Subgoal planning pseudocode
\begin{algorithm}
  \caption{Plan with Knowledge Base of Subgoals}
  \begin{algorithmic}[1]
    \State subgoalSequence $\gets$ BFS(subgoalKB, goal)
    \State plan = []
    \State curState $\gets$ subgoalSequence.pop()
    \For {subgoal $\in$ subgoalSequence}
    		\State plan += ValueIteration(curState, subgoal)
		\State curState $\gets$ plan.getLastState()
    \EndFor \\
    \Return plan;
  \end{algorithmic}
\end{algorithm}

In the case of \texttt{BRIDGEWORLD}, we might consider placing 
a block somewhere along the trench to be a subgoal. Then, we 
run Value Iteration to get from the agent's starting location to the 
point at which we've placed a block in the trench, stop, and run 
Value Iteration again from the first subgoal to the finish. There are 
a few problems with this approach, however.

% Maybe consider inserting Branavan's chart (or similar), or discuss how the search in subgoal space works?

{\bf Problem 1: Loss of generality}  One important thing to note about subgoals 
that {\it are} general enough to enhance an agent's planning abilities in a wide 
variety of state spaces is that they propose {\it necessary} claims about the 
domain that the agent occupies. If the subgoals are {\it contingent} (i.e. true in 
some state spaces of the domain but not in others), then they can be shown to 
completely lose their scalability. For instance, consider the task in \texttt{BRIDGEWORLD}, 
in which the agent must place a block in the trench that separates the agent from the goal. 
The subgoal $\langle blockInTrench, reachGoal\rangle$ might be a perfectly useful 
subgoal in \texttt{BRIDGEWORLD}, but an adversary could easily come up with 
thousands of worlds in which such a subgoal would completely derail the agent's 
planner. Thus, many subgoals do not scale beyond a particular instance of a state 
space. In order for subgoals to be useful, they must be necessary claims about the 
domain, otherwise, one can always come up with a counter world (by definition of necessary). 

{\bf Problem 2: Granular Planning} The second problem is that those that subgoals 
that do scale across state spaces are often not useful. For instance, the vast majority 
of tasks in Minecraft are not so easily broken into useful, necessary subgoals. 
Movement, for instance, is particularly difficult. As stated before, scalable subgoals 
must be necessary preconditions for a particular goal, and such preconditions are 
often difficult to come up with in a way that actually makes planning easier. One 
idea would be to create the movement subgoal that the agent is one away from 
the goal - this is a poor choice however, since this subgoal is hardly useful unless 
we repeat it (i.e. the agent is one away from the next subgoal, and so on). The 
result is an extremely granular and low level planning system that is no better 
than standard Value Iteration. If necessary preconditions existed for many goal 
types, then subgoal planning would be a great approach. Unfortunately, coming 
up with such subgoals is not an easy task, and often the best we can do is to 
plan at such a low level that we lose any benefit of planning over subgoals to begin with

% FIGURE OF PROBLEM 2

{\bf Problem 3: Researching the Space} The last problem with subgoal planning 
is that the use of subgoals actually requires that we research a huge portion of 
the state space. Consider the \texttt{BRIDGEWORLD} example in which the 
subgoal is to place a block along the trench somewhere - once we plan from the 
state in which a block has been placed at the trench, we research the entire first 
side of the trench. This problem only magnifies as you add more subgoals.

\begin{figure}
\centering
\includegraphics[scale=0.22]{figures/bridgeworld_sg.png}
\caption{The agent re-explores a large portion of the state space 
once it finds $\mathcal{SG}_1$. Also note that this subgoal 
highlights {\bf Problem 1}, in that it would be useless in many other Minecraft state spaces}
\end{figure}


A final, but less significant problem, is that Subgoal planning still 
requires the use of Value Iteration, which does not scale well - if 
there is ever a case in which planning between two subgoals is 
at all complex, then Subgoal planning is out of luck.

\subsection{OPTIONS}

The options framework proposes incorporating high-level policies to 
accomplish specific sub tasks. For instance, when an agent is near 
a door, the agent can engage the `door-opening-option-policy', which 
switches from the standard high-level planner to running a policy 
that is hand crafted to open doors. An option $o$ is defined as follows:

$o\ =\ \langle \pi_0, I_0, \beta_0\rangle$, where:

\begin{itemize}
\item[] $\pi_0 : (s,a) \rightarrow [0,1]$
\item[] $I_0 : s \rightarrow \{0,1\}$
\item[] $\beta_0 : s \rightarrow [0,1]$
\end{itemize}

Here, $\pi_0$ represents the {\it option policy}, $I_0$ represents
a precondition, under which the option policy may initiate, and 
$\beta_0$ represent the post condition, which determines which 
states terminate the execution of the option policy.

As Konidaris and Barto point out, the classic options framework is not 
generalizable, as it does not enable an agent to transfer knowledge from 
one state space to another. Recently, Konidaris and Barto's ~\citep{konidaris07} 
expand on the classic options framework and allow for a more portable 
implementation of options. Still, though, planning with options requires either 
that we plan in a mixed space of actions {\it and} options (which blows up the 
size of the search space), or requires that we plan entirely in the space of options. 
Additionally, providing an agent with an option policy is a difficult task for a human 
designer (especially if we want an optimal policy, which we do).

\subsection{MACROACTIONS}

\[
\boxed{\text{Running Example}}
\]

\section{AFFORDANCES}

%% Formalism
Formally, an Affordance is defined as: \\ \\
\vspace{1 mm} $\Delta =\ \langle p,g\rangle\ \longrightarrow \alpha$, where:

\begin{itemize}
\item[] $\alpha \subseteq \mathcal{A}$
\item[] $p : s \longrightarrow \{$0$, 1\}$
\item[] $g : s \longrightarrow \{$0$,1\}$
\end{itemize}

Where $\alpha$ is a subset of the agent's given set of actions 
$\mathcal{A}$, $p$ is a {\it precondition} that is a predicate over states, 
and $g$ is a {\it goal} or {\it subgoal} that is also a predicate over states.

The intuition is that in a huge number of planning scenarios, given a goal, the agent 
should be able to focus on only a subset of its available actions. The result is that the 
state-action space that the agent explores is astronomically smaller than in standard 
Value Iteration (especially in domains where the agent can change the environment 
to the degree of Minecraft). This parallels the intuition of Gibson's concept of an 
affordance, in which a human is capable of trimming down his or her considered 
action space by a huge amount when directed toward a particular goal. For instance, 
consider an agent with the standard Minecraft action set seen in Figure \ref{fig:all_actions} - 
if the agent need only walk across a flat surface to reach the goal, it should not 
even bother trying to place blocks or destroy blocks. If it needs to dig a ten block 
hole, then the agent should not consider movement or placement.

The reason that each goal encodes information about the goal relevant to 
those actions is that it, given perfect subgoal knowledge for a particular 
planning task, the affordance formalism will find an optimal policy {\it extremely}
 quickly. We imagine extensions in which an agent gets stuck and must ask 
 a human partner for help using natural language, and the resulting dialogue 
 could endow the agent with subgoal knowledge. This also allows the agent 
 to prune way unnecessary actions in $\mathcal{A}$ in each specific planning 
 task, making it possible to solve a engage with a large number of planning 
 scenarios that may call for different actions. Furthermore, since actions may 
 be pruned with respect to a given goal, agents may be endowed with huge 
 action sets that enable them solve a variety of problems across variable 
 state-spaces, yet the branching factor of an affordance agent's exploration 
 will be significantly smaller, since actions that are not relevant to the current 
 goal will be pruned. This makes the affordance formalism extremely robust, 
 as well as transferrable relative to subgoal planning and options.

% Insert pseudcode

\begin{algorithm}
  \caption{Plan with Knowledge Base of Affordances}

  \begin{algorithmic}
    \State \Comment{Reachability Analysis}
    \State currAs $\gets$ affordKB.pruneAs(initS, d.goal, actions)
    \State allStateActions $\gets$ $\langle State ,\langle Action \rangle \rangle$
    \For {s in allStateActions}
	\State nextAs $\gets$ affordKB.pruneAs(s, d.goal, allStateActions $\langle s \rangle$)
	\State allStateActions.add(s.do(nextAs))
    \EndFor
    \State \Comment{ValueIteration}
    \State policy $\gets$ ValueIteration(d, allStateActions)
    
    \Return policy;
  \end{algorithmic}
\end{algorithm}


The Affordance formalism introduced above and expanded on in this 
paper resolves the weaknesses of these other frameworks by limiting 
the complexity of the seed knowledge required of the designer, while 
still providing enough knowledge to limit the search space but also 
maintain scalability.

\section{EXPERIMENTS}

We conducted a series of experiments in the Minecraft domain that tested 
each planning system on a variety of tasks, ranging from basic path planning,
 to baking bread, to opening doors and jumping over trenches.  We also tested 
 each planner on worlds of varying size to demonstrate the scalability of each 
 system. In Table \ref{results_1}, we provide results on testing the planners 
 across each system. For each data point, we ran the planning system 3 times 
 and took the average result (they rarely deviated beyond $\frac{1}{10}$ of a second).

For the scenarios listed in Table \ref{results_1}, we limited the agent's 
action set across all planners so that it did not have access to block 
placement ($\square$) and block destruction ($\boxtimes$). The reason 
for excluding these two action types up is that, in Minecraft, the agent 
has an extraordinary ability to modify the state space via placing and 
destroying blocks. As a result, if an agent begins placing or destroying 
blocks in cases where it does not need to, the state-action space will 
explode exponentially and grow far too fast for (almost) any planner to 
finish in our lifetime. 

Consider that the agent is capable of destroying and placing blocks 
in a 10x10x2 world; there are on the order of:

\begin{equation}
O\left(\sum_{n=1}^{10 \cdot 10 \cdot 2} \binom{10 \cdot 10 \cdot 2}{n}\right)
\label{eq:mc_explode}
\end{equation}

states, which is far too large to explore. We will demonstrate that our 
affordance model {\it is} capable of handling these types of actions, 
and can plan using them as in a bridge building scenario or tunnel 
digging scenario (among others) while the other planners cannot. 
In fact, with these actions, none of the other planning systems 
can solve even the most basic path planning (even on just a flat 
surface with no obstacles). We explore this ability in the results 
seen in Table \ref{results_2}

As discussed above, one of the major advantages of using Affordances 
to plan is that they enable an agent to have a massive action set, 
making Affordances effectively transfer between domains. We 
conducted an additional set of experiments on the 15x15 world 
with no obstacles \texttt{15WORLD}. For this round of testing, we 
varied the number of actions available to the agent (starting from 
$|\mathcal{A}| = 4$ up to $|\mathcal{A}| = 25$) and ran the planner 
on \texttt{15WORLD} with the same goal (to reach the goal in the corner).


An additional advantage of planning with Affordances is that the 
problems of block-placement and block-destruction illustrated by 
\ref{eq:mc_explode} are overcome. With Affordances, we are able 
to solve a variety of novel planning problems in the Minecraft 
domain, such as building a bridge to cross a long trench, or digging 
a hole through a wall to reach the goal (see Table \ref{results_2}). 
This is indeed a compelling result, as no other planning system is 
currently able to avoid falling prey to the state-space explosion 
mentioned above. Additionally, the malleability of Minecraft that 
causes this explosion is a reasonable model of the way that an 
agent in the real world is capable of modifying its surroundings. 
Thus, we foresee the Affordance planner as being extremely deft 
at handling real world planning scenarios.


\section{RESULTS}

\begin{tabular}{ l || c | c | c }
  & Affordances & Subgoals & VI \\
  \hline
  \texttt{10WORLD} 		& ? 		& 	1.8s 		& 1.1s  \\
  \texttt{13WORLD} 		& ? 		& 	10.1s 	& 6.0s  \\
  \texttt{15WORLD} 		& ? 		& 	21.6s 	& 11.8s  \\
  \texttt{17WORLD} 		& ? 		& 	45.4s 	& 28.2s  \\
  \texttt{20WORLD} 		& ? 		& 	144.3s 	& 140.5s  \\
  \texttt{JUMPWORLD}  	& ? 		& 	21.1s 	& 10.1s \\
  \texttt{BREADWORLD}  	& ? 		& 	22.8s 	& 51.6s \\
  \texttt{DOORWORLD}  	& ? 		& 	? 		& 25.3s \\
  \texttt{MAZEWORLD}  	& ? 		& 	?		& 37.6s \\
  \texttt{HARDWORLD} 	& ? 		& 	? 		& 149.7s
  \label{results_1}
  \caption{Tests on a variety of tasks without block placement and destruction actions}
\end{tabular} 

\vspace{4 mm}

\begin{tabular}{ l || c | c | c }
  & Affordances & Subgoals & VI \\
  \hline
  $|\mathcal{A}| = 4$ 		& 	? 	& 	? 		& ?  \\
  $|\mathcal{A}| = 8$ 		& 	? 	& 	? 		& ?  \\
  $|\mathcal{A}| = 12$ 	& 	? 	& 	? 		& ?  \\
  $|\mathcal{A}| = 16$ 	& 	? 	& 	? 		& ?  \\
  $|\mathcal{A}| = 17$ 	& 	? 	& 	? 		& ?  \\
  $|\mathcal{A}| = 21$ 	& 	? 	& 	DNF 		& DNF  \\
  $|\mathcal{A}| = 25$ 	& 	? 	& 	DNF 		& DNF  \\
    \label{results_2}
    \caption{Tests on the simplest possible task (path planning in a flat plane with no obstacle) with incrementally larger action sets.}
\end{tabular}

As one can see from Table \ref{results_2}, in those cases where $|\mathcal{A}| = 21$ and $|\mathcal{A}| = 25$, the only planning algorithm to actually complete the tasks was the Affordance planner. This is because each of these cases scaled to include block destruction and block placement actions. Thus, any case in which these actions are required to complete the task at hand, only Affordance planning will succeed. This is significant, as Table \ref{results_1} indicates that the Affordance planner plans more effectively than the other systems, but it can also handle novel problems involving those actions that alter the environment in sever ways. We also include a Bonus round indicating those tasks that only the Affordance planner was able to solve. Finally, since each Affordance is attached to a particular goal, a single knowledge base will scale across state-spaces and task types, causing Affordance planning to be extremely transferrable.

% Insert bonus round of:
%	- Ladder!!!!
%	- Bridge
%	- Tunnel
%	- Builder tower under self

\section{CONCLUSION}

We proposed a novel approach to representing
knowledge in terms of {\em
  affordances}~\citep{gibson77} that allows an agent to efficiently prune its
action space based on domain knowledge. This pruning was shown to significantly
reduce the number of state/action pairs the agent needs to evaluate
in order to act optimally, and resulted in faster planning than subgoal planning, options,
and vanilla value iteration. We demonstrated the efficacy as well as the 
transferability of the affordance model in a series of planning tasks in the Minecraft domain.
In the future, we hope to learn affordances from experience as opposed to providing them directly
to the agent. Additionally, we hope to introduce uncertainty into the action set that is 
pruned, in order to improve the effectiveness of the pruning. Lastly, we 
hope to incorporate aid from a human partner through natural language dialogue, in which
the agent may ask for help when it is stuck and receive subgoal {\it hints} from a human
companion.

\bibliographystyle{plainnat}
\bibliography{main}  


\end{document}
