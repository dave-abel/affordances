\documentclass[]{article} 
\usepackage{proceed2e}

\usepackage[numbers,sort]{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{algorithm}
\usepackage{algpseudocode}

\title{Planning with Affordances}

\begin{document}
\author{}
\maketitle

\begin{abstract}
Current methods for exactly solving decision-making under uncertainty
require exhaustive enumeration of all possible states and actions,
leading to exponential run times, leading to the well-known ``curse of
dimensionality.''  Approaches to address this problem by providing the
system with formally encoded knowledge such as options or
macro-actions still fail to prevent the system from considering many
actions which seem obviously irrelevant for a human partner.  To
address this issue, we introduce a novel approach to representing
knowledge about how to plan in terms of {\em
  affordances}~\citep{gibson77}.  Our affordance formalism and
associated planning framework allows an agent to efficiently prune its
action space based on domain knowledge.  This pruning significantly
reduces the number of state/action pairs the agent needs to evaluate
in order to act optimally.  We demonstrate our approach in the
Minecraft domain on several planning and building tasks, showing a
significant increase in speed and reduction in state-space exploration
compared to subgoal planning, options, and macro-actions.
\end{abstract}

\section{INTRODUCTION}

As robots move out of the lab and into the real world, planning
algorithms need to be able to scale to domains of increased noise,
size, and complexity.  A classic formalization of this issue is the
sequential decision making problem, where increases in problem size
and complexity directly correspond to an explosion in the state-action
space. Current approaches to solving sequential decision making
problems cannot tackle these problems as the state-action space
becomes large~\citep{grounds05}.

There is a strong need for a generalizable form of knowledge that,
when coupled with a planner, is capable of solving problems in these
massive domains. Humans provide an excellent existence proof for such
planning, as we are capable of searching over an immense number of
possible actions when presented with a goal.  One approach to
explaining how humans solve this planning problem is by focusing on
problem-specific aspects of the environment which focus the search
toward the most relevant and useful parts of the state-action space.
Formally, \citet{gibson77} proposed to define this intuition as an
{\em affordance}, ``what [the environment] offers [an] animal, what
[the environment] provides or furnishes, either for good or ill.''
Additionally, roboticists have recently become interested in
leveraging affordances for perception and prediction of
humans~\citep{koppula13a, koppula13b}.

\begin{figure}
\centering
\includegraphics[scale = 0.18]{figures/bridgeworld_aff.png}
\caption{In the above Minecraft planning problem \texttt{BRIDGEWORLD}, the agent must place a block in the trench in order to reach the goal. This problem was only solvable by the Affordance planner.\label{fig:bridgeworld_aff}}
\end{figure}

In this paper we will formalize
the notion of an affordance as a piece of planning knowledge provided
to an agent operating in a Markov Decision Process
(MDP)~\citep{kaelbling99}.  We demonstrate that, like an option or
macro-action, an affordance provides additional information to the
agent, enabling more efficient planning.  However, unlike previous
approaches, an affordance enables more significant speedups by
reducing the size and branching-factor of the search space, enabling
an agent to focus its search on the most relevant part of the problem
at hand.  This approach means that a {\em single} set of affordances
provides general domain knowledge, becoming relevant just when the
agent reasons that it needs to pursue a particular subgoal.  Furthermore,
Affordances are not specific to a particular state-space nor problem-type, and thus, provide
the agent with transferrable knowledge that is effective in a wide variety of
domains and problems, unlike other approaches.


\begin{figure}
\centering
\includegraphics[scale = 0.15]{figures/all_actions.png}
\caption{The set of all actions in the Minecraft domain \label{fig:all_actions}}
\end{figure}

\section{BACKGROUND}

\subsection{OO-MDP}

The Object Oriented Markov Decision Process (OO-MDP) ~\citep{diuk08} is an extension of the classic Markov Decision Process (MDP), a fundamental building block of Reinforcement Learning (RL). RL is an algorithmic approach to sequential decision making problems, or more simply: planning.

A finite MDP is a five-tuple: $\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma \rangle$, where $\mathcal{S}$ is a state-space, $\mathcal{A}$ is the agent's set of actions, $\mathcal{T}$ denotes $\mathcal{T}[s' \mid s,a]$, the transition probability of an agent applying action $a \in \mathcal{A}$ in state $s \in \mathcal{S}$ and arriving in $s' \in \mathcal{S}$, and $\mathcal{R}(s,a)$ denotes the reward at $s$ when action $a$ is applied, and $\gamma$ is a discount factor. The OO-MDP changes the underlying representation of the state space $S$ so that worlds consist of instances of objects that are grouped into classes, where each object obtains of a set of attributes. Thus, a state in the OO-MDP is just the union of the attribute values of all objects in the world. As with a classical finite MDP, planning with an OO-MDP involves running Value Iteration to determine a policy. Recall that value iteration involves propagating reward throughout the state space until propagation has converged, at which point, a policy may be deduced. Reward propagation occurs as in a Bellman update:
\begin{equation}
U_{i+1}(s) \leftarrow \mathcal{R}(s) + \gamma \max_{a \in \mathcal{A}(s)} \sum_{s'} \text{Pr}(s' \mid s, a)U_i(s')
\end{equation}

Where $U_i(s)$ is the {\it utility} of state $s$ at iteration $i$, representing the expected reward of being in that state.

In practice, basic Value Iteration scales very poorly, either as the state space grows, or the action set grows. This is because the state-action space, depending on the domain, grows exponentially w.r.t the number of ways that the agent can change the environment. This is ameliorated slightly by introducing the OO-MDP, but it still fails in just about all of the planning scenarios we introduce here. On \texttt{BRIDGEWORLD}, the planner was incapable of enumerating the state space prior to filling up a 2Gb Java heapspace, presented with a single block and minimal action set (it could only place blocks in front of it, and could not destroy blocks) and thus, failed outright.

The reason for this failure is that in classical Value Iteration, the agent tries to explore all states that result from applying every action in every state - this is downright silly, as in \texttt{BRIDGEWORLD}, the agent will inevitably end up in the corner of the room placing and destroying blocks. This is an especially bad tactic in the Minecraft domain, as block placement results in a combinatoric explosion of the state space. Thus, in these scenarios, our Affordance planner has a substantial advantage on classic Value Iteration. These weaknesses are well known ~\citep{grounds05}, and have resulted in attempts to make planning more practical in domains of a larger scale.

% Value Iteration
% More details on OO-MDP, Value Iteration, etc.

\begin{figure}
\centering
\includegraphics[scale = 0.2]{figures/bridgeworld_vi.png}
\caption{Reachability while running VI}
\end{figure}

\subsection{SUBGOALS}
% % Intuition
Subgoal planning leverages the intuition that certain goals in planning domains may only be brought about if certain preconditions are first satisfied. For instance, in the Minecraft domain, one must be in possession of grain in order to bake bread. In Branavan et. al~\citep{branavan12a}, they explore learning subgoals from the Minecraft wiki and applying them in order to plan through a variety of problems in Minecraft.

% Formalism
Formally, in subgoal planning, the agent is set of subgoals, where each subgoal is a pair of predicates:

\[ 
SG = \langle x_k, x_l \rangle
\]

where $x_l$ is the effect of some action sequence performed on a state in which $x_k$ is true. Thus, subgoal planning requires that we perform high-level planning in subgoal space, and low-level planning to get from subgoal to subgoal.

% Insert pseudcode

\begin{algorithm}
  \caption{Plan with Knowledge Base of Subgoals}
  \begin{algorithmic}[1]
    \State subgoalSequence $\gets$ BFS(subgoalKB, goal)
    \State plan = []
    \State curState $\gets$ subgoalSequence.pop()
    \For {subgoal $\in$ subgoalSequence}
    		\State plan $\gets$ ValueIteration(curState, subgoal)
		\State curState $\gets$ plan.getLastState()
    \EndFor \\
    \Return plan;
  \end{algorithmic}
\end{algorithm}

In the case of \texttt{BRIDGEWORLD}, we might consider placing a block somewhere along the trench to be a subgoal. Then, we run Value Iteration to get from the agent's starting location to the point at which we've placed a block in the trench, stop, and run Value Iteration again from the first subgoal to the finish. There are a few problems with this approach, however.

% Maybe consider inserting Branavan's chart (or similar), or discuss how the search in subgoal space works?

{\bf Problem 1: Loss of generality}  One important thing to note about subgoals that {\it are} general enough to enhance an agent's planning abilities in a wide variety of state spaces is that they propose {\it necessary} claims about the domain that the agent occupies. If the subgoals are {\it contingent} (i.e. true in some state spaces of the domain but not in others), then they can be shown to completely lose their scalability. For instance, consider the task in \texttt{BRIDGEWORLD}, in which the agent must place a block in the trench that separates the agent from the goal. The subgoal $\langle blockInTrench, reachGoal\rangle$ might be a perfectly useful subgoal in \texttt{BRIDGEWORLD}, but an adversary could easily come up with thousands of worlds in which such a subgoal would completely derail the agent's planner. Thus, many subgoals do not scale beyond a particular instance of a state space. In order for subgoals to be useful, they must be necessary claims about the domain, otherwise, one can always come up with a counter world (by definition of necessary). 

{\bf Problem 2: Granular Planning} The second problem is that those that subgoals that do scale across state spaces are often not useful. For instance, the vast majority of tasks in Minecraft are not so easily broken into useful, necessary subgoals. Movement, for instance, is particularly difficult. As stated before, scalable subgoals must be necessary preconditions for a particular goal, and such preconditions are often difficult to come up with in a way that actually makes planning easier. One idea would be to create the movement subgoal that the agent is one away from the goal - this is a poor choice however, since this subgoal is hardly useful unless we repeat it (i.e. the agent is one away from the next subgoal, and so on). The result is an extremely granular and low level planning system that is no better than standard Value Iteration. If necessary preconditions existed for many goal types, then subgoal planning would be a great approach. Unfortunately, coming up with such subgoals is not an easy task, and often the best we can do is to plan at such a low level that we lose any benefit of planning over subgoals to begin with

% FIGURE OF PROBLEM 2

{\bf Problem 3: Researching the Space} The last problem with subgoal planning is that the use of subgoals actually requires that we research a huge portion of the state space. Consider the \texttt{BRIDGEWORLD} example in which the subgoal is to place a block along the trench somewhere - once we plan from the state in which a block has been placed at the trench, we research the entire first side of the trench. This problem only magnifies as you add more subgoals.

\begin{figure}
\centering
\includegraphics[scale=0.22]{figures/bridgeworld_sg.png}
\caption{The agent re-explores a large portion of the state space once it finds $\mathcal{SG}_1$. Also note that this subgoal highlights {\bf Problem 1}, in that it would be useless in many other Minecraft state spaces}
\end{figure}


A final, but less significant problem, is that Subgoal planning still requires the use of Value Iteration, which does not scale well - if there is ever a case in which planning between two subgoals is at all complex, then Subgoal planning is out of luck.

\subsection{OPTIONS}

The options framework proposes incorporating high-level policies to accomplish specific sub tasks. For instance, when an agent is near a door, the agent can engage the `door-opening-option-policy', which switches from the standard high-level planner to running a policy that is hand crafted to open doors. An option $o$ is defined as follows:

$o\ =\ \langle \pi_0, I_0, \beta_0\rangle$, where:

\begin{itemize}
\item[] $\pi_0 : (s,a) \rightarrow [0,1]$
\item[] $I_0 : s \rightarrow \{0,1\}$
\item[] $\beta_0 : s \rightarrow [0,1]$
\end{itemize}

Here, $\pi_0$ represents the {\it option policy}, $I_0$ represents a precondition, under which the option policy may initiate, and $\beta_0$ represent the post condition, which determines which states terminate the execution of the option policy.

As Konidaris and Barto point out, the classic options framework is not generalizable, as it does not enable an agent to transfer knowledge from one state space to another. Recently, Konidaris and Barto's ~\citep{konidaris07} expand on the classic options framework and allow for a more portable implementation of options. Still, though, planning with options requires either that we plan in a mixed space of actions {\it and} options (which blows up the size of the search space), or requires that we plan entirely in the space of options. Additionally, providing an agent with an option policy is a difficult task for a human designer (especially if we want an optimal policy, which we do).

\subsection{MACROACTIONS}

\[
\boxed{\text{Running Example}}
\]

\section{AFFORDANCES}

%% Formalism
Formally, an Affordance is defined as: \\ \\
\vspace{1 mm} $\Delta =\ \langle p,g\rangle\ \longrightarrow \alpha$, where:

\begin{itemize}
\item[] $\alpha \subseteq \mathcal{A}$
\item[] $p : s \longrightarrow \{$0$, 1\}$
\item[] $g : s \longrightarrow \{$0$,1\}$
\end{itemize}

Where $\alpha$ is a subset of the agent's given set of actions $\mathcal{A}$, $p$ is a {\it precondition} that is a predicate over states, and $g$ is a {\it goal} or {\it subgoal} that is also a predicate over states.

The intuition is that in a huge number of planning scenarios, given a goal, the agent should be able to focus on only a subset of its available actions. The result is that the state-action space that the agent explores is astronomically smaller than in standard Value Iteration (especially in domains where the agent can change the environment to the degree of Minecraft). This parallels the intuition of Gibson's concept of an affordance, in which a human is capable of trimming down his or her considered action space by a huge amount when directed toward a particular goal. For instance, consider an agent with the standard Minecraft action set seen in Figure \ref{fig:all_actions} - if the agent need only walk across a flat surface to reach the goal, it should not even bother trying to place blocks or destroy blocks. If it needs to dig a ten block hole, then the agent should not consider movement or placement.

The reason that each goal encodes information about the goal relevant to those actions is that it, given perfect subgoal knowledge for a particular planning task, the affordance formalism will find an optimal policy {\it extremely} quickly. We imagine extensions in which an agent gets stuck and must ask a human partner for help using natural language, and the resulting dialogue could endow the agent with subgoal knowledge. This also allows the agent to prune way unnecessary actions in $\mathcal{A}$ in each specific planning task, making it possible to solve a engage with a large number of planning scenarios that may call for different actions.

Most importantly, within the Affordance framework, agent's may have {\it huge} action sets, since they will effectively prune their action set as needed. As a result, Affordance-equipped agents are able to solve a variety of problems across variable state-spaces with a single set of knowledge. This makes affordances extremely robust, as well as transferrable relative to subgoal planning and options.

% Insert pseudcode

\begin{algorithm}
  \caption{Plan with Knowledge Base of Affordances}

  \begin{algorithmic}
    \State \Comment{Reachability Analysis}
    \State currAs $\gets$ affordKB.pruneAs(initS, d.goal, actions)
    \State allStateActions $\gets$ $\langle State ,\langle Action \rangle \rangle$
    \For {s in allStateActions}
	\State nextAs $\gets$ affordKB.pruneAs(s, d.goal, allStateActions $\langle s \rangle$)
	\State allStateActions.add(s.do(nextAs))
    \EndFor
    \State \Comment{ValueIteration}
    \State policy $\gets$ ValueIteration(d, allStateActions)
    
    \Return policy;
  \end{algorithmic}
\end{algorithm}


The Affordance formalism introduced above and expanded on in this paper resolves the weaknesses of these other frameworks by limiting the complexity of the seed knowledge required of the designer, while still providing enough knowledge to limit the search space but also maintain scalability.

We should be able to prove that given a ``good" set of subgoals the agent will be able to reach one after the other with high probability. Therefore, in the case that the agent cannot reach a subgoal there is likely a better one. The agent should then prompt for a more specific subgoal that will better allow it to reach the next one.

\section{EXPERIMENTS}

\begin{figure}
\centering
\includegraphics[scale=0.2]{figures/reg_actions.png}
\caption{The pruned set of actions provided to the agent for tasks 4-8 in the Knowledge Round \label{fig:reg_actions}}
\end{figure}

We tested each planning algorithm (vanilla Value Iteration, Subgoal planning, Options, Affordance planning) on seven distinct tasks, divided into two groups, the Block group (requires block placement or destruction), and the General group (tasks that do not require block placement or destruction). We issue these experiments in two rounds, the Knowledge Round, and the Transfer Round.

\boxed{\begin{tabular}{ l | l }
{\bf Block Group} & {\bf General Group} \\
\hline \\
B1: \texttt{FLATWORLD} & G1: \texttt{SMALLPATHWORLD} \\
B2: \texttt{BRIDGEWORLD10} & G2: \texttt{WALLWORLD} \\
B3: \texttt{TUNNELWORLD} & G3: \texttt{DOORWORLD} \\
& G4: \texttt{BIGWORLD} \\

\end{tabular}}


In the {\bf Knowledge Round}, Options, Subgoals, Macro-actions, and Affordances were all given a hand crafted set of knowledge for each individual task. This round is intended to demonstrate the efficacy of each planning system, given perfect knowledge for a particular planning scenario. For tasks 1-3, the agent was given the set of actions seen in Figure \ref{fig:all_actions} (includes block placement and destruction) and one block to place. For tasks 4-8, the agent was given the set of actions described in Figure\ref{fig:reg_actions} (does not include block placement or destruction). The reason for providing these two action sets and splitting the tasks up is that in Minecraft, the agent has an extraordinary ability to modify the state space via placing and destroying blocks. As a result, if an agent begins placing or destroying blocks in cases where it does not need to, the state-action space will explode exponentially and grow far too fast for the planner to finish in our lifetime (for just about any planner). 

Consider that the agent is capable of destroying and placing blocks in a 10x10x2 world; there are on the order of:

\begin{equation}
O\left(\prod_{n=1}^{10*10*2} \binom{10*10*2}{n}\right)
\label{eq:mc_explode}
\end{equation}

states, which is far too large to explore. We will demonstrate that our affordance model {\it is} capable of handling such actions, and can plan using them as in a bridge building scenario or tunnel digging scenario (among others) while the other planners cannot. In fact, with these actions, none of the other planning systems can solve even the most basic path planning from (see {\bf B1: \texttt{FLATWORLD}}). However, in order to also prove the generality of affordance planning (and not just demonstrate that it takes advantage of the Minecraft explosion from Equation \ref{eq:mc_explode}, we also test on planning scenarios in which the agent is given a more normal set of actions (found in Figure \ref{fig:reg_actions}).

In the {\bf Transfer Round}, we required that each system use one set of knowledge to plan through each of the eight tasks. This is intended to measure the transferability of each planning technique. Additionally, we provide the agent with the set of all actions so that it can actually solve the problems, the set in Figure. \ref{fig:all_actions}.
For each iteration, we measured the wall-clock time to find the goal.

The set of tasks were inspired by those planning scenarios that the creators of each other planning system proposed.
\subsection{Block Group}

Recall that the action set for these tasks is defined in Figure \ref{fig:all_actions}.

{\bf B1: \texttt{FLATWORLD}} A 10x10 world with no obstacles.

\begin{center}
\includegraphics[scale=0.24]{figures/flatland.png}
\end{center}

Actions are defined as in Figure \ref{fig:all_actions}
	
{\bf B2: \texttt{BRIDGEWORLD10}} A 10x10 world with a trench cutting across the center, separating the agent from the goal. The agent must build a bridge (i.e. place two blocks across the trench) to reach the goal.

\begin{center}
\includegraphics[scale=0.28]{figures/bridgeworld10.png}
\end{center}
% Knowledge base provided for each in ROUND A

{\bf B3: \texttt{TUNNELWORLD}}

%\begin{center}
%\includegraphics[scale=0.28]{figures/tunnelworld.png}
%\end{center}

% Knowledge base provided for each in ROUND A

\subsection{General Group}

Recall that the action set for these tasks is defined in Figure \ref{fig:reg_actions}.

{\bf G1: \texttt{SMALLPATHWORLD}} A 10x10 world with a trench cutting across the middle separating the agent from the goal with a small bridge connecting the two sides.

%\begin{center}
%\includegraphics[scale=0.23]{figures/smallpathworld.png}
%\end{center}

{\bf G2: \texttt{WALLWORLD}} A 10x10 world in which the agent is nearly surrounded by an indestructible wall (i.e. it cannot destroy the grey blocks) and must first move farther away from the goal in order to reach the goal.

\begin{center}
\includegraphics[scale=0.21]{figures/uwall.png}
\end{center}

{\bf G3: \texttt{DOORWORLD}} A 10x10 world in which the agent must open a door to reach the goal.

%\begin{center}
%\includegraphics[scale=0.21]{figures/doorworld.png}
%\end{center}


%

% Knowledge base provided for each in ROUND A



{\bf G5: \texttt{MEDWORLD}} Identical to {\bf B1: \texttt{FLATWORLD}}, but the dimensions of the world are 15x15.


{\bf G5: \texttt{BIGWORLD}} Identical to {\bf B1: \texttt{FLATWORLD}}, but the dimensions of the world are 20x20.

% Knowledge base provided for each in ROUND A


%{\bf G5: \texttt{LIGHTWORLD}} This is an adaptation of one of Konidaris' examples demonstrating the efficacy of portable options. A 10x10 world where the agent is surrounded on all sides by indestructible walls (i.e. the agent cannot destroy the grey blocks). The agent must press the button in order to open the door and proceed to the goal.
%
%\begin{center}
%\includegraphics[scale=0.23]{figures/lightworld.png}
%\end{center}
% ---- Also include the knowledge base provided for each in ROUND B, where that KB must solve all 5 tasks

\section{RESULTS}

\subsection{KNOWLEDGE ROUND:}

As expected, all of the other planning types failed in reaching the goal (they each filled up a 1.5Gb java heap in 45 minutes and had not yet even finished reachability).

\begin{tabular}{ l || c | c | c | c }
  & Affordances & Options & Subgoals & VI \\
  \hline
  {\bf M1} & 1s & ? & DNF & DNF  \\
  {\bf M2} & 40s & ? & DNF & DNF \\
  {\bf M3} & 40s & ? & DNF & DNF \\
  {\bf G1} & 1s & ? & 1s & 1s \\
  {\bf G2} & 1s & ? & 4s & 2s \\
  {\bf G3} & 1s & ? & 1s & 1s \\
  {\bf G4} & 1s & ? & 13s & 8s \\
  {\bf G4} & 1s & ? & 13s & 8s \\
\end{tabular}


\subsection{TRANSFER ROUND:}

Figure NEED FIGURE indicates the knowledge that we provided each system. We did our best to provide each system with optimal knowledge, in the sense that these knowledge bases ought to do better than any other. Below are the results for the transfer round.

\begin{figure}
\centering
\boxed{\begin{tabular}{ l c }
{\it AffordanceKB} $=$ & \{\} \\
Subgoals & \{\} \\
Options & \{\} \\
ValueIteration & \{\} \\
\end{tabular}}
\caption{The knowledge bases provided to each planning system to solve all five tasks \label{tbl:transfer_kb}}
\end{figure}


\begin{tabular}{ l || c | c | c | c }
  & Affordances & Options & Subgoals & VI \\
  \hline
  {\bf M1} & 1s & ? & DNF & DNF  \\
  {\bf M2} & 15s & ? & DNF & DNF \\
  {\bf M3} & 40s & ? & DNF & DNF \\
  {\bf G1} & 1s & ? & 1s & 1s \\
  {\bf G2} & 1s & ? & 4s & 2s \\
  {\bf G3} & 1s & ? & 1s & 1s \\
  {\bf G4} & 1s & ? & 13s & 8s \\
\end{tabular}

% As well as some charts indicating search space size, #cycles, etc..

\section{CONCLUSION}

We proposed a novel approach to representing
knowledge in terms of {\em
  affordances}~\citep{gibson77} that allows an agent to efficiently prune its
action space based on domain knowledge. This pruning was shown to significantly
reduce the number of state/action pairs the agent needs to evaluate
in order to act optimally, and resulted in faster planning than subgoal planning, options,
and vanilla value iteration. We demonstrated the efficacy as well as the 
transferability of the affordance model in a series of planning tasks in the Minecraft domain.
In the future, we hope to learn affordances from experience as opposed to assuming
that they are given by a human partner. Additionally, we hope to introduce some uncertainty
into the action set that is pruned, in order to improve the effectiveness of the pruning. Lastly,
we hope to incorporate aid from a human partner through natural language dialogue, in which
the agent may ask for help when it is stuck and receive subgoal {\it hints} from a human
companion.

\bibliographystyle{plainnat}
\bibliography{main}  


\end{document}
