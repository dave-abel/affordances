\documentclass[]{article} 

\usepackage{subfigure}
\usepackage[numbers,sort]{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{ifthen,version}
\newboolean{include-notes}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{geometry}
\usepackage{bm}
%\usepackage{adjustbox}
\usepackage[usenames,dvipsnames]{color}
\newcommand{\stnote}[1]{\textcolor{Blue}{\textbf{ST: #1}}}
\newcommand{\jmnote}[1]{\textcolor{Green}{\textbf{JM: #1}}}
\newcommand{\dnote}[1]{\textcolor{Orange}{\textbf{D: #1}}}
\newcommand{\gnote}[1]{\textcolor{Red}{\textbf{G: #1}}}


\title{\textbf{Learning with Dirichlet Priors for \\Affordance Aware Planning}}

\begin{document}
\author{
\textbf{David Abel, Gabriel Barth-Maron} \\ \textbf{James MacGlashan, Stefanie Tellex} \\
Brown University \\
Providence, RI, USA \\
\texttt{\{dabel, gabrielbm, jmacglashan, stefie10\}@cs.brown.edu}}

\date{}
\maketitle

\begin{abstract}
Planning algorithms for non-deterministic domains are often
intractable in large state spaces due to the well-known ``curse of
dimensionality.'' In previous work, we introduced a novel, state- and reward- general approach to limiting the
branching factor in large domains by reasoning about the
domain in terms of {\em affordances}~\citep{gibson77}.  Our affordance
formalism can be coupled with a variety of planning frameworks to
create ``affordance aware planning,'' allowing an agent to efficiently
search the state-space by focusing on relevant action possibilities. 


\[
\Delta_i = \langle p, g \rangle \longmapsto \lambda
\]

\[
A = \{a_1, a_2, \ldots, a_n, o_1, o_2, \ldots, o_m\}
\]

%This corresponds to highlighting useful actions on a state by state basis when solving an MDP.


% In addition, we determine the number of useful actions for each affordance by generating a set of optimal trajectories taken from the training policies to form a Dirichlet distribution over the size of  useful actions for each affordance. 
  
Previously, we provided planners with affordances, leading to massive
speed ups in planning compared to algorithms without affordances. 
To avoid hand crafting knowledge we propose learning affordances through scaffolding~\citep{bruner76}. We randomly generate many simplified state spaces that are
representative of more complicated environments the agent will plan over.

We propose forming policies over each of the simplified state spaces and using these policies to learn a
Dirichlet-Multinomial distribution over each affordance's action set. In addition, we propose sampling optimal trajectories from the trained policies to form a Dirichlet distribution over the number of useful actions for each affordance. During evaluation, for each state the agent visits we sample from the latter distribution to infer how many useful action possibilities will be available to it. This process maximizes the probability of an affordance focusing the agent on the most relevant actions.

% This is done to ensure that the learned distribution  \jmnote{I'm not sure it will be entirely clear what you're doing here. Something probably needs to be said about looking at the frequency of optimal actions for each possible affordance to model what the important actions are. I'm also not sure I understand why trajectories are analyzed {\em} in addition to the policy, since the former is a superset of the latter.}

We are still collecting experimental data on hard-coded vs learned affordances. We believe that the learned
affordances will perform slightly worse than hand crafted affordances, and substantially better than planners
without affordances. 
\end{abstract}
{\small
\bibliographystyle{plainnat}
\bibliography{../the_paper/uai/main}  
}

\end{document}