\documentclass[11pt]{article} 
\input{p.tex}
\title{Affordance Math}
\usepackage{fullpage}
\usepackage{natbib}
\date{}
\begin{document}

\maketitle

\section{Affordances}
Affordances are defined as a mapping from an OO-MDP state $s$ to a set of actions $\mathcal{A}$. \\

Each affordance also maintains a tuple $\langle p, g, M, n\rangle$, where:

\begin{itemize}
\item $p$ is a predicate on states, representing the precondition for activating the affordance
\item $g$ is a goal type, representing the type of problem the agent is solving.
\item $M$ is a multinomial over actions.
\item $n$ an integer indicating the size of the action set.
\end{itemize}

The mapping occurs by evaluating the precondition $p$ in state $s$, and determining if the goal type
is currently relevant (direct equivalence to current goal, logical entailment of goal types, etc), and if so,
returning an action set $\mathcal{A}$. The action set is either provided by a domain expert, or is learned and applied as follows. \\

\section{Computing $\mathcal{A}$}
$\mathcal{A}$ is the result of taking $n$ samples from a multinomial over actions:

\begin{equation}
\mathcal{A} \leftarrow_n Mult(A)
\end{equation}

Where $A$ is the OO-MDP action set. This multinomial, and $n$, may be specified by a domain expert, forcing $\mathcal{A}$ to be deterministic if the expert chooses. \\

If an expert is not involved, then we need to learn two things:
\begin{enumerate}
\item The multinomial over actions to sample from
\item How many actions to sample from the multinomial
\end{enumerate}

\section{Learning}

\noindent {\bf Our goal is to learn the multinomial and $n$ for each affordance so that the overall action set contributed by the set of affordances for a given state is identical to the optimal action set for that state} \\

\noindent We define $\mathcal{A}^*$ to be the set of of optimal actions for each state $s$, and define $\mathcal{K}^*$ to be the action set suggested by the affordance knowledge base for state $s$. \\

\noindent Succinctly, we want to learn the affordances that maximize the following probability:

\begin{equation}
\Pr(\mathcal{K^*} = \mathcal{A}^* | s, \Delta_1 \dots \Delta_K)
\label{eq:master_eq}
\end{equation}

\noindent Since each affordance $\Delta_i$ maps to a set of actions for a given state $s$, $\mathcal{K}^*$ is defined as:

\begin{equation}
\mathcal{K}^* = \Delta_1(s) \cup \ldots \cup \Delta_K(s)
\end{equation}
\vspace{2mm}

\noindent More specifically:
\begin{enumerate}
\item[1)] Each action in $\mathcal{A}^*$ is in $\mathcal{K}^*$
\item[2)] Each action {\it not} in $\mathcal{A}^*$ is {\it not} in $\mathcal{K}^*$.
\end{enumerate}
\begin{align}
\Pr(\mathcal{K^*} = \mathcal{A}^* | s, \Delta_1 \dots \Delta_K)
= \left[\prod_i^{|\mathcal{A}^*|} \sum_j^{|\Delta|} \Pr(a_i \in \Delta_j(s) \mid s, \Delta_j)\right] \cdot
\left[1 - \prod_i^{|\bar{\mathcal{A}^*}|} \sum_j^{|\Delta|} \Pr(b_i \in \Delta_j(s) \mid s, \Delta_j)\right]
\label{eq:isin_notin}
\end{align}

\noindent Where $a_i$ is the $i$-th action in $\mathcal{A}^*$, and $b_i$ is the $i$-th action in $\bar{\mathcal{A}^*}$ (the set of non-optimal actions). \\

\noindent Equation \ref{eq:isin_notin} represents the probability that each optimal action is in the action set contributed by the affordances, and that each non-optimal action is not in the action set contributed by the affordances. \\

\noindent Furthermore, we define the probability that the $i$-th action is in the given affordances action set to be:
\begin{equation}
\Pr(a_i \in \Delta_j(s) \mid s, \Delta_j) = DirMult(\Delta_j.\alpha, \Delta_j.n)
\end{equation}

\noindent Where $\Delta_j.\alpha$ denotes the hyper parameter vector for the Dirichlet-multinomial, and $\Delta_j.n$ indicates how many samples to draw. We define $\Delta_j.n$ to be a sample from a Dirichlet distribution over the affordances hyper parameter vector $\beta$:

\begin{equation}
\Delta_j.n \sim Dir(\Delta_j.\beta)
\end{equation}

\noindent Therefore, in order to solve Equation \ref{eq:master_eq}, we need supply the Dirichlet-multinomial hyper parameters $\Delta_j.\alpha$ and $\Delta_j.\beta$. These are precisely the parameters that are learned during training.

\noindent We learn these parameters by synthesizing the optimal policy in several smaller worlds that parallel the test world, but are small enough to be solved with tabular methods. From this policy, we receive the optimal action set, $\mathcal{B}^*$ for each state for each of those worlds.

\noindent Because of the state-space dependence of the OO-MDP, we make the assumption that the optimal actions in similar states in the training worlds will be similar to the optimal action set in similar states in the test world. For more information on how these parameters are computed, see the pseudocode of the algorithms in the paper.

{\small
\bibliographystyle{plainnat}
\bibliography{main}  
}

\end{document}
