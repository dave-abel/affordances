\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{subfloat}
\usepackage{float}
\usepackage{mathtools}% http://ctan.org/pkg/mathtools
\usepackage{amsmath,empheq}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}
\usepackage[usenames,dvipsnames]{color}
\usepackage{tikz}
\usepackage{pgfplots}

% -- Comment commands --
 \newcommand{\stnote}[1]{\textcolor{Blue}{\textbf{stefie10: #1}}}
 \newcommand{\dnote}[1]{\textcolor{Green}{\textbf{dabel:  #1}}}
 \newcommand{\enote}[1]{\textcolor{Red}{\textbf{ellis:  #1}}}
 \newcommand{\gnote}[1]{\textcolor{Purple}{\textbf{gabe:  #1}}}
 \newcommand{\jnote}[1]{\textcolor{Orange}{\textbf{james:  #1}}}

% -- Misc. new commands --
\newcommand{\argmax}{\operatornamewithlimits{argmax}} % argmax
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}} % booktabs table cmd

\begin{document}

% paper title
\title{Affordance-Aware Planning}

% Author info:

\maketitle

\begin{abstract}
Planning algorithms for non-deterministic domains are often
intractable in large state spaces due to the well-known curse of
dimensionality. Existing approaches to planning in large stochastic
state spaces fail to prevent autonomous agents from considering many
actions that are obviously irrelevant to a human solving the same
task. To prevent agents from applying irrelevant actions we formalize the notion of {\em affordances} as
state space independent, goal-oriented knowledge added to an Object Oriented Markov Decision
Process (OO-MDP).  Affordances prune irrelevant actions based on the agent's goal and
the current state, reducing the number of state-action pairs
the planner must evaluate in order to formulate a near optimal
policy. Moreover, affordances may be provided by an expert or may be learned without supervision.
We demonstrate our approach planning in the state-rich Minecraft domain, showing significant increases in
speed, reductions in state space exploration, and improvements in the quality of the synthesized policy.
Additionally, we show that learned affordances often surpass the
performance of those provided by experts. We also demonstrate that
affordance-aware planning enables a Baxter robot to assist a person performing a cooking task.
\end{abstract}

%\maketitle

% ====== Section: Introduction ======
\section{Introduction}
\label{sec:introduction}

Robots operating in unstructured, stochastic environments such as a
factory floor or a kitchen face a difficult planning problem due to
the large state space and inherent uncertainty due to unreliable
perception and actuation~\citep{bollini12,knepper13}.  Robotic
planning tasks are often formalized as a stochastic sequential
decision making problem, modeled as a Markov Decision Process
(MDP)~\citep{thrun2008probabilistic}. In these problems, the agent
must find a mapping from states to actions for some subset of the
state space that enables the agent to achieve a goal while minimizing
costs along the way. However, many robotics tasks are so complex that
modeling them as an MDP results in a massive state-action space, which
in turn restricts the types of robotics problems that are
computationally tractable. For example, when a robot is manipulating
objects in an environment, an object can be placed anywhere in a large
set of locations. The size of the state space increases exponentially
with the number of objects, which bounds the placement problems that
the robot is able to expediently solve. Moreover, the difficulty of the task is compounded
by the fact that most of these objects and locations are irrelevant. For instance, when making brownies,
the oven and flour are important, while the soy sauce and saut\'{e}
pan are not.  For a different task, such as stir-frying broccoli, a
different set of objects and actions are relevant.

To address this state-action space explosion, prior work has explored
adding knowledge to the planner, such as options~\cite{sutton99} and
macroactions~\cite{Botea:2005kx,Newton:2005vn}.  However, while these
methods allow the agent to search more deeply in the state space, they
add non-primitive actions to the planner which {\em increase} the size of
the state-action space.  The resulting augmented space is even larger,
which can have the paradoxical effect of increasing the search time
for a good policy~\cite{Jong:2008zr}.  
Deterministic forward-search algorithms like hierarchical task
networks (HTNs)~\citep{Nau:1999:SSH:1624312.1624357}, and temporal
logical planning (TLPlan)~\citep{Bacchus95usingtemporal,Bacchus99usingtemporal},
add knowledge to the planner that greatly increases planning speed, but do
not generalize to stochastic domains. Additionally, the knowledge
provided to the planner by these methods is quite extensive, reducing the
agent's autonomy.

Instead, we augment an Object Oriented Markov Decision Process (OO-MDP) with a formalization of {\em
  affordances}. Affordances were originally proposed by \citet{gibson77} as
action possibilities prescribed by an agent's capabilities in an environment.
%Affordances focus an agent's attention on aspects of the environment that
%are most relevant to solving its current goal and avoid exploration of irrelevant parts of the
%world.
We rigorously formalize the notion of an affordance as knowledge added
to an OO-MDP that prunes irrelevant actions on a state by state basis.
Our affordances can be specified by hand and also learned through experience,
making them a concise, transferable, and learnable means of
representing useful planning knowledge. Our experiments demonstrate
that affordances provide dramatic improvements for a variety of planning
tasks compared to baselines, and apply across different state spaces.
Moreover, while manually provided affordances outperform baselines, affordances
learned through experience yield even greater improvements. 
We conduct experiments in the game Minecraft, which has a very
large state-action space, and on a real-world robotic cooking assistant.

%% Because affordances define the {\em kind} of goals for which actions
%% are useful, affordances also enable high-level reasoning that can be
%% combined with approaches like subgoal planning for even
%% greater performance gains. 

% ====== Affordances ======
\section{Technical Approach}
\label{sec:affordances}

An MDP is a five-tuple: $\langle \mathcal{S}, \mathcal{A},
\mathcal{T}, \mathcal{R}, \gamma \rangle$, where $\mathcal{S}$ is a
state space; $\mathcal{A}$ is the agent's set of actions;
$\mathcal{T}$ denotes $\mathcal{T}(s' \mid s,a)$, the transition
probability of an agent applying action $a \in \mathcal{A}$ in state
$s \in \mathcal{S}$ and arriving in $s' \in \mathcal{S}$;
$\mathcal{R}(s,a,s')$ denotes the reward received by the agent for
applying action $a$ in state $s$ and and transitioning to state $s'$;
and $\gamma \in [0, 1)$ is a discount factor that defines how much the
agent prefers immediate rewards over future rewards (the agent
prefers to maximize immediate rewards as $\gamma$ decreases).

Our representation of affordances builds on Object-Oriented MDPs
(OO-MDP)~\citep{diuk08}.  An OO-MDP efficiently represents the state
of an MDP through the use of objects and predicates.  An OO-MDP state
is a collection of objects, $O = \{o_1, \ldots, o_o \}$.  Each object
$o_i$ belongs to a class, $c_j \in \{c_1, \ldots, c_c\}$. Every class
has a set of attributes, $Att(c) = \{c.a_1, \ldots, c.a_a \}$, each of
which has a domain, $Dom(c.a)$, of possible values. OO-MDPs enable
planners to use predicates over classes of objects. That is, the
OO-MDP definition also includes a set of predicates $\mathcal{P}$ that
operate on the state of objects to provide additional high-level
information about the MDP state.

OO-MDP predicates provide state space independence. For a given planning domain, OO-MDP objects
often appear across tasks. Since predicates operate on collections
of objects, they generalize beyond specific state spaces within the given domain.
For instance, in Minecraft, a predicate checking the contents of the agent's inventory
generalizes beyond any particular Minecraft task. We capitalize on this state space
independence by using OO-MDP predicates as features for action pruning.

% -- Subsection: Modeling the Optimal Actions --
\subsection{Modeling the Optimal Actions}

%% \citet{gibson77} proposed that an affordance is ``what [the
%%   environment] offers [an] animal, what [the environment] provides or
%% furnishes, either for good or ill.''  

Our goal is to formalize affordances in a way that enables a planning
algorithm to prune away suboptimal actions in each state. We define
the optimal action set, $\mathcal{A}^*$, for a given state $s$ and
goal $G$ as:
% -- Equation: Optimal Action Set --
\begin{equation}
\mathcal{A}^* = \left\{ a \mid Q^*_G(s,a) = V^*_G(s) \right\}, 
\label{eq:opt_act_set}
\end{equation}
where, $Q^*_G(s,a)$ and $V^*_G(s)$ represent the optimal Q function and 
value function, respectively.

We aim to learn a probability distribution over the optimality of each action
for a given state ($s$), goal ($G$), and knowledge base ($K$) from
which action pruning may be informed. Thus, we want to infer a Bernouli
for each action's optimality:
% -- Equation: Master Equation --
\begin{equation}
\Pr(a_i \in \mathcal{A}^* \mid s, G, K)
\label{eq:master}
\end{equation}

\noindent for $i \in \{1, \ldots, |\mathcal{A}|\}$, where $\mathcal{A}$ is the OO-MDP action space.
%Henceforth, we simplify the event $a_i \in \mathcal{A}^*$ to $a_i$, and $a_i \not\in \mathcal{A}^*$ to $\neg a_i$. 

We formalize our knowledge base, $K$, as a set of $n$ paired
preconditions and goal types, $\{ (p_1, g_1) \ldots (p_{n}, g_{n})\}$,
along with a parameter vector, $\theta$. We abbreviate each pair
$(p_j, g_j)$ to $\delta_j$ for simplicity. Each precondition $p \in
\mathcal{P}$ is a {\it predicate} in predicate space, $\mathcal{P}$,
defined by the OO-MDP, and $g \in \mathcal{G}$ is a {\it goal type} in
goal space.  For example, a predicate might be $nearTrench(agent)$
which is true when the agent is standing near a trench. We assume
that the goal space consists of logical expressions of state
predicates. A goal type specifies the sort of problem the agent is
trying to achieve. In the context of Minecraft, a goal type might
refer to the agent retrieving an object of a certain type from the
environment, reaching a particular location, or crafting an object or structure.
Depending on the agent's current goal, the relevance of each action changes
dramatically.

We rewrite Equation~\ref{eq:master} replacing $K$ with its constituents:
% -- Equation: replace K --
\begin{multline}
\Pr(a_i \mid s, G, K) \\
= \Pr(a_i \in \mathcal{A}^* \mid s, G, \delta_1 \ldots \delta_n, \theta_i)
\end{multline}
\noindent where $\theta_i$ represents the set of parameters relevant to modeling
the probability of action $a_i \in {\cal A}^*$. 

We introduce the indicator function $f$, which returns 1 if and only if $\delta_j$'s predicate is true in the provided state $s$, and $\delta_j$'s goal type matches the agent's current goal, $G$:
% -- Equation: function f defn --
\begin{equation}
f(\delta, s, G) = 
\begin{cases}
1& \delta.p(s) \wedge (G == \delta.g) \\
0& \text{otherwise}
\end{cases}
\label{eq:f_func_def}
\end{equation}

Evaluating $f$ for each $\delta_j$ given the current state and goal gives rise to a set of binary features,
$\phi_j = f(\delta_j, s, G)$, which we use to reformulate our probability distribution.
% -- Equation: replace deltas with phi --
\begin{multline}
\Pr(a_i \in \mathcal{A}^*  \mid s, G, \delta_1 \ldots \delta_n, \theta_i) \\
= \Pr(a_i \in \mathcal{A}^*  \mid \phi_1, \ldots, \phi_n, \theta_i)
\label{eq:feature_rep}
\end{multline}

This distribution may be modeled in a number of ways making this
approach quite flexible. However, to enable efficient learning, we model our distribution using Naive
Bayes. First we factor using Bayes' rule:
% -- Equation: Bayes --
\begin{equation}
= \frac{\Pr(\phi_1, \ldots, \phi_{n}, \mid a_i \in \mathcal{A}^*, \theta_i) \Pr(a_i \in \mathcal{A}^* \mid \theta_i)}{\Pr(\phi_1, \ldots, \phi_{n} | \theta_i)}
%= \frac{\Pr(\phi_1, \ldots, \phi_{n}, \mid a_i, \theta_i) \Pr(a_i \mid \theta_i)}{\Pr(\phi_1, \ldots, \phi_{n} | \theta_i)}
\label{eq:bayes}
\end{equation}

Next we assume that each feature is conditionally independent of the others, given whether the action is optimal:
% -- Equation: Naive assumption and uniform prior--
\begin{equation}
= \frac{\prod_{j=1}^{n} \Pr(\phi_j \mid a_i \in \mathcal{A}^*, \theta_i) \Pr(a_i \in \mathcal{A}^* \mid \theta_i) }{\Pr(\phi_1, \ldots, \phi_{n} | \theta_i)}
\label{eq:final}
\end{equation}

Finally, we define the prior on the optimality of each action to be the fraction of the
time each action was optimal during training. This distribution fully describes the model used by our affordance-aware planner. 

% -- Subsection: Learning the Optimal Actions--
\subsection{Learning the Optimal Actions}
Our approach to modeling the optimality of each action allows affordances to be learned through unsupervised experience.
To learn affordances, we provide a set of training worlds ($W$) for which the optimal policy, $\pi$,
may be tractably computed. Then, we compute the maximimum likelihood estimate of the parameter vector $\theta_i$ for each action.
%\begin{equation}
%\argmax_{\theta_i} \sum_{w \in W} \sum_{s \in w} \Pr( a_i \mid s, w.G, \pi, \theta_i)
%\end{equation}
%where $w.G$ is the goal that was solved in world $w$.

Under our Bernouli Naive Bayes model, we estimate the parameters
$\theta_{i,0} = \Pr(a_i)$ and $\theta_{i,j} = \Pr(\phi_j | a_i)$, for $j \in \{1, \ldots, n \}$, where the maximum likelihood estimates are:
\begin{align}
\theta_{i,0} &= \frac{C(a_i)}{C(a_i) + C(\bar{a_i})} \\
\theta_{i,j} &= \frac{C(\phi_j, a_i)}{C(a_i)}
\end{align}

Here, $C(a_i)$ is the number of observed occurrences where $a_i$ was optimal across all worlds $W$,
$C(\bar{a_i})$ is the number of observed occurrences where $a_i$ was not optimal,
and $C(\phi_j, a_i)$ is the number of occurrences where $\phi_j=1$ and $a_i$ was optimal.

\begin{align}
C(a_i) &= \sum_{w \in W} \sum_{s \in w} (\pi(s) == a_i) \\
C(\bar{a_i}) &= \sum_{w \in W} \sum_{s \in w} (\pi(s) \not= a_i) \\
C(\phi_j, a_i) &= \sum_{w \in W} (\sum_{s \in w} \pi(s) == a_i \wedge \phi_j == 1)
\end{align}

In all cases, optimality was determined according to $\pi$.

% -- Subsection: Action Pruning --
\subsection{Affordance-Aware Planning}
\label{sec:action_pruning}
We complete our formalization of affordances by exploring three different ways to prune actions
based on the distribution on the optimality of each action.

% 1) Expert (discriminative or model)
First, affordances may be specified by a domain expert in place of the Naive Bayes. In this approach,
an expert specifies a set of actions associated with a
precondition-goal type pair. When the precondition is active and goal type is the same
as the agent's present goal, the actions suggested by the affordance are included
in the agent's action set. For instance, if an agent is standing
above a block of buried gold and is trying to smelt a block of gold,
then an expert may indicate that the agent should consider the actions
of looking down and digging. Table~\ref{table:afford_kb_exp} shows several examples of expert
defined affordances. All actions contributed by active affordances are
grouped to yield the set of actions to consider for each state.

% 2) Threshold
Second, actions may pruned by thresholding the posterior.
In this method, the affordances remove any
actions whose probability of being optimal is below the provided
threshold for each state. The threshold was determined empirically, and was set to
$\frac{0.2}{|\mathcal{A}|}$, where $|\mathcal{A}|$ is the size of the
full action space of the OO-MDP.  This threshold is quite
conservative, and means that our approach only prunes actions which
are extremely unlikely to be optimal.

% 3) Sampling
Lastly, actions may be pruned by sampling actions from the
probability distribution as specified by
Equation~\ref{eq:final}. We treat each action's probability mass as a
Bernouli trial and sample across the entire action set. In preliminary
results, this method did not perform as well as baselines - likely
because the weights associated with each action were too small. In
future work, we are interested in investigating more sophisticated
approaches to pruning actions with sampling.

Through the use of any of the above methods, an affordance-aware
planner prunes actions on a state by state basis, focusing the agent
on relevant action possibilities of the environment, consequently
reducing planning time. Any planner operating in an OO-MDP may be made
affordance-aware with this approach.

% -- Table: Expert provided affordances --
\begin{table}[b]
\ra{1.35}
\begin{tabular}{@{}llll@{}}\toprule
Precondition & Goal Type & Actions \\ \midrule
\texttt{lookingTowardGoal} & \texttt{atLocation} & \texttt{\{move\}} \\
\texttt{lavaInFront} & \texttt{atLocation} & \texttt{\{rotate\}} \\
\texttt{lookingAtGold} & \texttt{hasGoldOre} & \texttt{\{destroy\}} \\
\bottomrule
\end{tabular}

\caption{Examples of Expert Provided Affordances\label{table:afford_kb_exp}}
\end{table}

In a recent review on the theory of affordances,~\citet{chemero2003}
suggests that an affordance is a relation between the features of an
environment and an agent's abilities. Our approach grounds this
interpretation, where the features of the environment correspond to
the goal-dependent state features, $\phi$, and the agent's abilities
correspond to the OO-MDP action set. In our model, there is an
affordance for each $\delta_j$, with preconditions $\delta_j.p$, goal
type $\delta_j.g$ and action distribution $\Pr(a_i \in \mathcal{A}^* | \phi_j,
\theta)$, which is computed in our Naive Bayes model by marginalizing
over all the features not associated with $\phi_j$.

%As with human agents, multiple affordances often inform decision
%making at a given time.  Thus, affordance-aware planning agents
%operating within an OO-MDP will rarely make specific reference to
%particular affordances, but will instead reason about the world using
%the relevant action possibilities identified by the distribution in
%Equation~\ref{eq:final}.  That said, individual affordances can still
%be specified by experts, providing a natural spectrum between
%expert-provided knowledge and learned knowledge.

% -- Figure: Minecraft pic --
\begin{figure}
\centering
\includegraphics[scale=0.08]{figures/smelting_labeled.png}%
  \caption{A gold smelting task in the Minecraft domain.  The agent's
    goal is to mine a block of gold, move to the forge and then smelt
    the gold in the forge to produce gold ingots.}
  \label{fig:minecraft}
\end{figure}

% ====== Section: Results ======
\section{Results}
\label{sec:results}

% -- Figure: Average results --
\begin{figure*}
\centering
\includegraphics[scale=0.18]{figures/average_results_cropped.png}%
\caption{Average Results From All Maps}
\label{fig:average_results}
\end{figure*}

We evaluate our approach using the game Minecraft and a
collaborative robotic cooking task.  Minecraft is a 3-D blocks game in which
the user can place, craft, and destroy blocks of different types.
Minecraft's physics and action space allow users to create complex
systems, including logic gates and functional scientific graphing
calculators\footnote{https://www.youtube.com/watch?v=wgJfVRhotlQ}.
Minecraft serves as a model for robotic tasks such as cooking
assistance, assembling items in a factory, object retrieval, and
complex terrain traversal.  As in these tasks, the agent operates in a
very large state-action space in an uncertain environment.
Figure~\ref{fig:minecraft} shows a scene from one of our Minecraft
problems. \dnote{Stephen: describe your domain here}

% -- Subsection: Minecraft Tests --
\subsection{Minecraft Tests}

Our experiments consisted of five common tasks in Minecraft, including
constructing bridges over trenches, smelting gold, tunneling
through walls, basic path planning, and digging to find an object.  We tested on 
randomized worlds of varying size and difficulty. The generated test
worlds varied in size from tens of thousands of states to hundreds of thousands of states.

For learned affordances, a knowledge base was derived from the training data, which consisted of 20 simple state spaces of each map type
(100 total maps), each approximately a 1,000-10,000 state world. We
conducted all tests with a single knowledge base. 

We use Real Time Dynamic Programming
(RTDP)~\cite{barto95} as our baseline planner, a sampling-based algorithm that does
not require the planner to visit all states. We compare RTDP with learned affordance-aware RTDP (LRTDP), and
expert-defined affordance-aware RTDP (ERTDP). LRTDP pruned actions according to the thresholded pruning method.
We terminated each planner when the maximum change in the value
function was less than 0.01 for 100 consecutive policy rollouts, or
the planner failed to converge after 1000 rollouts.  We set the reward
function to $-1$ for all transitions, except transitions to states in
which the agent was in lava, where we set the reward to $-10$. The goal was set to
be terminal. The discount factor was set to $\lambda = 0.99$. For all
experiments, movement actions (move, rotate, jump) had a small
probability (0.05) of incorrectly applying a different movement
action.

The evaluation metrics for each trial were the number of Bellman updates
executed by each planning algorithm, the accumulated reward
of the average plan, and the CPU time taken to find a
plan. Table~\ref{table:minecraft_results} shows the average Bellman
updates, accumulated reward, and CPU time for RTDP, LRTDP and ERTDP
after planning in 20 different maps of each goal type (100
total). 

\begin{table}[H]
\ra{1.25}
\begin{tabular}{@{}llll@{}}\toprule
Planner & Bellman & Reward & CPU \\ \midrule
\texttt{RTDP}   			&	27439 ($\pm$2348)		&	-22.6 ($\pm$9)		& 107 ($\pm$33) \\
\texttt{LRTDP} 			& 	{\bf 9935} ($\pm$1031)	&	{\bf -12.4} ($\pm$1)& {\bf 53} ($\pm$5) \\ \hline
\texttt{RTDP+Opt}  		&	26663 ($\pm$2298)		&	-17.4 ($\pm$4) 		& 129($\pm$35) \\
\texttt{LRTDP+Opt} 		& 	{\bf 9675} ($\pm$953)	&	{\bf -11.5} ($\pm$1)	&{\bf 93} ($\pm$10) \\ \hline
\texttt{RTDP+MA}  		&	31083 ($\pm$2468)		&	-21.7	 ($\pm$5)		&336 ($\pm$28) \\
\texttt{LRTDP+MA}  		& 	{\bf 9854} ($\pm$1034)	&	{\bf -11.7} ($\pm$1)	&{\bf 162} ($\pm$17) \\ %\hline
%\texttt{RTDP+MA+Opt}  	&	27143($\pm$2380)		&	-16.9($\pm$3.0)	&	323($\pm$38)		\\ 
%\texttt{LRTDP+MA+Opt} & 	{\bf 10622($\pm$1046)}	&	{\bf -13.4($\pm$1.0)}	&	{\bf 237($\pm$29)}		\\
\bottomrule
\end{tabular}
\caption{Affordances vs. Temporally Extended Actions}
\label{table:temp_ext_act_results}
\end{table}

Figure~\ref{fig:average_results} shows the results averaged
across all maps. The full results for these trials are shown in Table~\ref{table:minecraft_results}.
Because the planners were forced to terminate after
only 1000 rollouts, they did not always converge to the optimal
policy. As the results suggest, LRTDP on average found a comparably
better plan (10.6 cost) than ERTDP (22.7 cost)  and RTDP (36.4 cost), found
the plan in significantly fewer Bellman updates (14287.5 to ERTDP's 24804.1
and RTDP's 34694.3) and in less CPU time (93.1s to ERTDP's 166.4s and RTDP's 242.0s).  These
results indicate that while learned affordances gave the largest
improvements, expert-provided affordances can also significantly
enhance performance, and, depending on the domain, could add
significant value in making large state-spaces tractable without the
overhead of supplying training worlds.

% -- Subsection: Temporally Extended Actions --
\subsection{Temporally Extended Actions and Affordances}

We compared our approach to Temporally Extended Actions: macroactions
and options. We conducted these experiments with the same
configurations as our Minecraft experiments. Domain experts provided
the option policies and macroactions.

Table~\ref{table:temp_ext_act_results} indicates the results of
comparing RTDP equipped with macro actions, options, and affordances
across 100 different executions in the same randomly generated
Minecraft worlds. The results are averaged across tasks of each
type presented in Table~\ref{table:minecraft_results}. As the results
suggest, both macroactions and options add a significant amount of
time to planning.  This increase is because it is computationally
expensive to predict the expected reward associated with applying an
option or a macroaction. Furthermore, the branching factor of the
state-action space significantly increases when augmented with
additional actions, causing the planner to run for longer and perform
more Bellman updates. With affordances, the planner found a better
plan in less CPU time, and with fewer Bellman updates. These results
support the claim that affordances can handle the augmented action
space provided by temporally extended actions by pruning away
unnecessary actions.

% -- Subsection: Baxter --
\subsection{ARTDP and Baxter}

\dnote{Stephen: Insert description of baxter stuff here}

% -- Figure: Baxter results/image
\begin{figure}[H]
\centering
\includegraphics[scale=0.195]{figures/baxter_temp.jpg}%
  \caption{Placeholder for baxter results/image}
  \label{fig:baxter_results}
\end{figure}

% ====== Section: Related Work ======
\section{Related Work}
\label{sec:related-work}

In this section, we discuss the differences between
affordance-aware planning and other forms of knowledge engineering that
have been used to accelerate planning. We divide these approaches
into those that are built to plan in stochastic domains, and those that are
designed for use with deterministic domains.

% -- Subsection: Stochastic --
\subsection{Stochastic Approaches}


\begin{table}[t]
\ra{1.2}
\begin{tabular}{@{}llll@{}}\toprule
Planner & Bellman & Reward & CPU \\ \midrule
&\hspace{-10mm}{\it Mining Task} \\
\texttt{RTDP} & 17142.1 ($\pm$3843) 		& {\bf -6.5} ($\pm$1)  & {\bf 17.6s}   ($\pm$4) \\
\texttt{ERTDP} 	& 14357.4 ($\pm$3275) 		& {\bf -6.5}   ($\pm$1) & 31.9s   ($\pm$8) \\
\texttt{LRTDP} 	& {\bf 12664.0} ($\pm$9340) 	& -12.7 ($\pm$5) & 33.1s   ($\pm$23) \\\hline
&\hspace{-10mm}{\it Smelting Task} \\
\texttt{RTDP} 	& 30995.0 ($\pm$6730) 		& {\bf -8.6}   ($\pm$1) & 45.1s   ($\pm$14) \\
\texttt{ERTDP} 	& 28544.0 ($\pm$5909) 		& {\bf -8.6}   ($\pm$1) & 72.6s   ($\pm$19) \\ 
\texttt{LRTDP} 	& {\bf 2821.9} 	 ($\pm$662) 	& -9.8   ($\pm$2) & {\bf 7.5s}  ($\pm$2) \\ \hline
&\hspace{-10mm}{\it Wall Traversal Task} \\
\texttt{RTDP} & 45041.7 ($\pm$11816) 		& -56.0   ($\pm$51) & {\bf 68.7s}   ($\pm$22) \\
\texttt{ERTDP} 	& 32552.0 ($\pm$10794) 		& -34.5   ($\pm$25) & 96.5s   ($\pm$39) \\ 
\texttt{LRTDP} 	& {\bf 24020.8} ($\pm$9239) 	& {\bf -15.8}   ($\pm$5) & 80.5s   ($\pm$34) \\ \hline
&\hspace{-10mm}{\it Trench Traversal Task} \\
\texttt{RTDP}  	& 16183.5 ($\pm$4509) 		& {\bf -8.1}   ($\pm$2) & 53.1s   ($\pm$22) \\
\texttt{ERTDP} 	& {\bf 8674.8} 	($\pm$2700) 	& -8.2   ($\pm$2) & {\bf 35.9s}   ($\pm$15) \\ 
\texttt{LRTDP} 	& 11758.4 ($\pm$2815) 		& -8.7   ($\pm$1) & 57.9s   ($\pm$20) \\ \hline
&\hspace{-10mm}{\it Plane Traversal Task} \\
\texttt{RTDP} & 52407 ($\pm$18432) 		& -82.6   ($\pm$42) & 877.0s   ($\pm$381) \\
\texttt{ERTDP} 	& 32928 ($\pm$14997) 		& -44.9   ($\pm$34) & 505.3s   ($\pm$304) \\
\texttt{LRTDP} 	& {\bf 19090} 	 ($\pm$9158) 	& {\bf-7.8}   ($\pm$1) & {\bf 246s}  ($\pm$159) \\
\bottomrule
\end{tabular}
\caption{RTDP vs. Affordance-Aware RTDP}
\label{table:minecraft_results}
\end{table}

Here, we compare other approaches of action pruning and
knowledge engineering that provide speedups to planners
in stochastic domains.

% --- Subsection: Temporally Extended Actions ---
\subsubsection{Temporally Extended Actions}
Temporally extended actions are actions that the agent can select like
any other action of the domain, except executing them results in
multiple primitive actions being executed in succession. Two common
forms of temporally extended actions are {\em
  macro-actions}~\cite{hauskrecht98} ~and {\em
  options}~\cite{sutton99}.  Macro-actions are actions that always
execute the same sequence of primitive actions. Options are defined
with high-level policies that accomplish specific sub tasks. For
instance, when an agent is near a door, the agent can engage the
`door-opening-option-policy', which switches from the standard
high-level planner to running a policy that is hand crafted to open
doors.  Although the classic options framework is not generalizable to
different state spaces, creating {\em portable} options is a topic of
active
research~\cite{konidaris07,konidaris2009efficient,Ravindran03analgebraic,croonenborghs2008learning,andre2002state,konidaris2012transfer}.

Since temporally extended actions may negatively impact planning
time~\cite{Jong:2008zr} by adding to the number of actions the agent
can choose from in a given state, combining affordances with
temporally extended actions allows for even further speedups in
planning, as demonstrated in Table~\ref{table:temp_ext_act_results}. In
other words, affordances are complementary knowledge to options and
macroactions.

% --- Subsection: Action Pruning ---
\subsubsection{Action Pruning}

Sherstov and Stone~\cite{sherstov2005improving} considered MDPs with a
very large action set and for which the action set of the optimal
policy of a source task could be transferred to a new, but similar,
target task to reduce the learning time required to find the optimal
policy in the target task. The main difference between our
affordance-based action set pruning and this action transfer work is
that affordances prune away actions on a state by state basis, whereas
the learned action pruning is on per task level. Further, with goal
types, affordances may be attached to subgoal planning for a
significant benefit in planning tasks where complete subgoal knowledge
is known.

Rosman and Ramamoorthy~\cite{rosman2012good} provide a method for learning action
priors over a set of related tasks. Specifically, they compute a Dirichlet distribution over 
actions by extracting the frequency that each action was optimal in each state for each 
previously solved task.

Action priors can only be used with planning/learning algorithms that
work well with an $\epsilon$-greedy rollout policy, while affordances
can be applied to almost any planning algorithm.  In addition, action
priors are only utilized for fraction $\epsilon$ of the time steps,
which is typically quite small, limiting the improvement they can make
to the planning speed.  Finally, as variance in tasks explored
increases, the priors will become more uniform. In contrast,
affordance-aware planning can handle a wide variety of tasks in a
single knowledge base, as demonstrated by
Table~\ref{table:minecraft_results}.

% --- Subsubsection: Heuristics ---
\subsubsection{Heuristics}
Heuristics in MDPs are used to convey information about the value of a
given state-action pair with respect to the task being solved and
typically take the form of either {\em value function initialization},
or {\em reward shaping}. Initializing the value function to an
admissible close approximation of the optimal value function has been
shown to be effective for LAO* and RTDP~\cite{Hansen:1999qf}.  Reward
shaping is an alternative approach to providing heuristics. The
planning algorithm uses a modified version of the reward function that
returns larger rewards for state-action pairs that are expected to be
useful, but does not guarantee convergence to an optimal policy unless
certain properties of the shaped reward are satisfied~\cite{potshap}.

However, heuristics are highly dependent on the reward function and
state space of the task being solved, whereas affordances are state
space independent and may be learned easily for different reward
functions. If a heuristic can be provided, the combination of
heuristics and affordances may even more greatly accelerate planning
algorithms than either approach alone.

% -- Subsection: Deterministic knowledge engineering approaches --
\subsection{Deterministic Approaches}

There have been several successful attempts at engineering knowledge
to decrease planning time for deterministic planners. These are
fundamentally solving a different problem from what we are interested
in since they deal with non-stochastic problems, but there are a
number of salient parallels and contrasts to be drawn nonetheless.

% --- Subsubsection: Hierarchical Task Networks ---
\subsubsection{Hierarchical Task Networks}
Traditional Hierarchical Task Networks (HTNs) employ \textit{task decompositions} to aid in planning~\cite{erol1994htn}. The goal at hand is decomposed into smaller tasks which are in turn decomposed into smaller tasks. This decomposition continues until primitive tasks that are immediately achievable are derived. The current state of the task decomposition, in turn, informs constraints which reduce the space over which the planner searches. At a high level both HTNs and affordances fulfill the same role: both achieve action pruning by exploiting some form of supplied knowledge. 

However there are two essential distinctions between affordances and traditional HTNs.  (1) HTNs do not incorporate reward into their planning. Consequently, they lack mathematical guarantees of optimal planning. (2) On a qualitative level, the degree of supplied knowledge in HTNs surpasses that of affordances: whereas affordances simply require relevant propositional functions, HTNs require not only constraints for sub-tasks but a hierarchical framework of arbitrary complexity.

% --- Subsubsection: Temporal Logic ---
\subsubsection{Temporal Logic}
Bacchus and Kabanza~\cite{Bacchus95usingtemporal,Bacchus99usingtemporal} provided
planners with domain dependent knowledge in the form of a first-order version of linear
temporal logic (LTL), which they used for control of a forward-chaining planner. With this methodology, 
a \textsc{Strips} style planner may be guided through the search space by checking 
whether candidate plans do not falsify a given knowledge base of LTL formulas, often
achieving polynomial time planning in exponential space.

The primary difference between this body of work and affordance-aware planning is that
affordances may be learned increasing autonomy of the agent and flexibility of the approach, while LTL formulas are far
too complicated to learn effectively, placing dependence on an expert.

% ====== Section: Conclusion ======
\section{Conclusion}
\label{sec:conclusion}
We proposed a novel approach to represent transferable knowledge in terms of
{\em affordances}~\cite{gibson77}. Our affordances allow an agent to efficiently prune actions 
based on learned knowledge, providing a significant reduction in the number of state-action
pairs the agent needs to evaluate in order to act near optimally. We demonstrated the 
effectiveness of the affordance model by comparing RTDP to its affordance-aware
equivalents in a series of challenging planning tasks in the Minecraft domain. Further, we designed
a full learning process that allows an agent to autonomously learn useful affordances that may be used
across a variety of task types, reward functions, and state spaces, allowing for convenient extensions 
to robotic applications.

Additionally, we compared the effectiveness of augmenting planners with affordances, with 
temporally extended actions, and the combination of the two. The results suggest that affordances may be combined with 
temporally extended actions to provide improvements in planning.

Lastly, we deployed an affordance-aware planner on a robot in a collaborative cooking task with a massive 
state space. \dnote{Stephen: more baxter details here}.

In the future, we hope to automatically discover useful state space specific subgoals online 
- a topic of some active research \cite{Mcgovern01automaticdiscovery,Simsek:2005:IUS:1102351.1102454}.
Automatic discovery of subgoals would allow affordance-aware planners to take advantage of the goal-oriented
focus of affordances, and would further reduce the size of the explored state-action space by improving the effectiveness of action pruning. 

Additionally, we hope to explore additional methods that capitalize on the distribution over optimal actions,
such as incorporating affordances with a forward search sparse sampling algorithm~\cite{walsh2010integrating},
or replacing the Naive Bayes model with a more sophisticated model, such as Logistic Regression or a Noisy-OR.
We are also investigating methods of learning the thresholded value in a more principled way - one such
approach is to initialize the planner with a strict threshold, and slowly relax the threshold until a near optimal
policy is found.

{\small
\bibliographystyle{plainnat}
\bibliography{main}
}
\end{document}


