\documentclass[conference]{IEEEtran}
\usepackage{times}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{subfloat}
\usepackage{amsmath,empheq}
\usepackage{amssymb}
\usepackage{latexsym}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}
\usepackage[usenames,dvipsnames]{color}
\newcommand{\stnote}[1]{\textcolor{Blue}{\textbf{ST: #1}}}
\newcommand{\dnote}[1]{\textcolor{Green}{\textbf{D: #1}}}
\newcommand{\gnote}[1]{\textcolor{Purple}{\textbf{G: #1}}}

\pdfinfo{
   /Author (David Abel  \& Gabriel Barth-Maron, James MacGlashan, Stefanie Tellex)
   /Title  (Toward Affordance-Aware Planning)
   /CreationDate (D:20101201120000)
   /Subject (Planning, Affordances, Sequential Decision Making)
   /Keywords (Planning, Affordance, MDP, Learning)
}

\begin{document}

% paper title
\title{Affordance-Aware Planning}

% Author info:
\author{\authorblockN{David Abel \& Gabriel Barth-Maron, James MacGlashan, Stefanie Tellex}
\authorblockA{Department of Computer Science, Brown University \\
\texttt{\{dabel,gabrielbm,jmacglashan,stefie10\}@cs.brown.edu}}}

\maketitle

\begin{abstract}
Planning algorithms for non-deterministic domains are often
intractable in large state spaces due to the well-known ``curse of
dimensionality.''  Existing approaches to address this problem fail to prevent autonomous agents
from considering many actions which would be obviously irrelevant to a
human solving the same problem. We formalize the notion of affordances
as knowledge added to an MDP that prunes actions in a state- and
reward- general way. This pruning significantly reduces the number of
state-action pairs the agent needs to evaluate in order to act
optimally. We demonstrate our approach in the Minecraft domain as a
model for robotic tasks, showing significant increase in speed and
reduction in state-space exploration during planning. Further, we
provide a learning framework that enables an agent to learn
affordances through experience, opening the door for agents to learn
to adapt and plan through new situations. We provide preliminary
results indicating that the learning process effectively produces
affordances that help solve an MDP faster, suggesting that affordances
serve as an effective, transferable piece of knowledge for planning
agents in large state spaces. We compare our approach with several
existing knowledge engineering methods and deploy an affordance-aware planner
on ReThink's Baxter as part of a cooperative cooking task.

\end{abstract}

\IEEEpeerreviewmaketitle

% ====== Section: Introduction ======
\section{Introduction}
\label{sec:introduction}

As robots move out of the lab and into the real world, planning
algorithms need to scale to domains of increased noise, size, and
complexity.  A classic formalization of this problem is a stochastic
sequential decision making problem in which the agent must find a
policy (a mapping from states to actions) for some subset of the state
space that enables the agent to achieve a goal from some initial
state, while minimizing any costs along the way.
Increases in planning problem size and complexity directly correspond
to an explosion in the state-action space, restricting extensions to large state-spaces
such as robotic applications. Current approaches to solving 
sequential decision making problems in the face of uncertainty cannot tackle these problems 
as the state-action space becomes too large~\cite{grounds05}.

To address this state-space explosion, prior work has explored adding
knowledge to the planner to solve problems in these
massive domains, such as options~\cite{sutton99} and
macroactions~\cite{Botea:2005kx,Newton:2005vn}. However, these
approaches add knowledge in the form of additional high-level actions
to the agent, which {\em increases} the size of the state-action space
(while also allowing the agent to search more deeply within the
space).  The resulting augmented space is even larger, which can have
the paradoxical effect of increasing the search time for a good
policy~\cite{Jong:2008zr}. Other approaches fall short of learning useful, transferable knowledge,
either due to complexity or lack of generalizability.

Instead, we propose a formalization of {\em affordances} \cite{gibson77} for Markov Decision Processes (MDPs) that
specifies which actions an agent should consider in different kinds of states to achieve a certain kind of goal.
Our approach enables an agent to focus on
aspects of the environment that are most relevant toward solving its current goal 
and avoids exploration of irrelevant parts of the 
state-action space, which leads to dramatic speedups in planning.

Further, we propose a learning process that enables agents to
autonomously learn affordances through experience, lessening the
agent's dependence on expert knowledge. Affordances are not specific
to a particular reward function or state space, and provide the agent
with transferable knowledge that is effective in a wide variety of
problems. We call any planner that uses affordances an {\it
  affordance-aware} planner.

Because affordances define the {\em kind} of goals for which actions
are useful, affordances also enable high-level reasoning that can be
combined with approaches like subgoal planning for even
greater performance gains. In our current model, ideal subgoals are
sometimes given directly to planning agents by an expert - however, we are
interested in automatically discovering subgoals in an online way, a
problem which has already enjoyed some success
\cite{Mcgovern01automaticdiscovery,Simsek:2005:IUS:1102351.1102454}.


% ====== Section: Background ======
\section{Background}
\label{sec:background}

We use Minecraft as our planning and evaluation domain. Minecraft is a
3-D blocks world game in which the user can place and destroy blocks
of different types.   It serves as a model for a variety of robotic tasks involving assembly and construction.  Minecraft's physics and action space are expressive
enough to allow very complex systems to be created by users, including logic gates and 
functional scientific graphing calculators\footnote{https://www.youtube.com/watch?v=wgJfVRhotlQ};
simple scenes from a Minecraft world appear in Figure~\ref{fig:epicworld} - a video demonstration of
an early iteration of an affordance-aware planner solving this task may be seen online\footnote{Watch at: https://vimeo.com/88689171}.
Minecraft serves as a model for robotic tasks such as cooking assistance, assembling items in a factory, 
and object retrieval.  As in these tasks, the agent operates in a very large state-action space in an uncertain environment.

\begin{figure*}
\centering
\subfigure[Start]{
\includegraphics[width=0.23\linewidth]{figures/epicworld_1.jpg}}%
\subfigure[Destroy Wall]{
\includegraphics[width=0.23\linewidth]{figures/epicworld_2.jpg}}%
\subfigure[Collect Ore]{
\includegraphics[width=0.23\linewidth]{figures/epicworld_3.jpg}}%
\subfigure[Smelt Ore]{
\includegraphics[width=0.23\linewidth]{figures/epicworld_4.jpg}}%
  \caption{Affordance-aware RTDP tasked with a gold-smelting task with a variety of obstacles
  (only solved by an affordance-aware planner)}
  \label{fig:epicworld}
\end{figure*}

Minecraft is also an effective parallel for the actual world, both
in terms of approximating the complexity and scope of planning
problems, as well as modeling the uncertainty and noise presented to a
robotic agent.  For instance, robotic agents are prone to
uncertainty throughout their system, including noise in their
sensors (cameras, LIDAR, microphones, etc.), odometry, control, and
actuation.  In order to accurately capture some of the inherent
difficulties of planning under uncertainty, the Minecraft agent's
actions were modified to have stochastic outcomes. These stochastic
outcomes may require important changes in the optimal policy in
contrast to deterministic actions, such as keeping the agent's
distance from high cost areas of the state-space, such as lava or cliffs.

We chose to give the Minecraft agent perfect sensor data about the Minecraft world.
However, affordances typically relate to the agent's immediate surroundings,
so limiting the perceptual scope should not impede the performance gains of affordances.
We have considered extensions to Partially Observable domains, though at a distance
solving a POMDP is effectively unchanged by the presence of affordances (beyond the
performance gains provided by pruning actions).

\subsection{OO-MDPs}

We define affordances in terms of propositional functions on states. Our definition builds on the Object-Oriented Markov Decision Process
(OO-MDP) \cite{diuk08}.  OO-MDPs are an extension of
the classic Markov Decision Process (MDP).  A classic MDP is a
five-tuple: $\langle \mathcal{S}, \mathcal{A}, \mathcal{T},
\mathcal{R}, \gamma \rangle$, where $\mathcal{S}$ is a state-space;
$\mathcal{A}$ is the agent's set of actions; $\mathcal{T}$ denotes
$\mathcal{T}(s' \mid s,a)$, the transition probability of an agent
applying action $a \in \mathcal{A}$ in state $s \in \mathcal{S}$ and
arriving in $s' \in \mathcal{S}$; $\mathcal{R}(s,a,s')$ denotes the
reward received by the agent for applying action $a$ in state $s$ and
and transitioning to state $s'$; and $\gamma \in [0, 1)$ is a discount
  factor that defines how much the agent prefers immediate rewards
  over distant rewards (the agent more greatly prefers to maximize
  more immediate rewards as $\gamma$ decreases).

A classic way to provide a factored representation of an MDP state is to represent
each MDP state as a single feature vector. By contrast, an OO-MDP represents the state space as a collection of objects,
$O = \{o_1, \ldots, o_o \}$.  Each object $o_i$ belongs to a
class $c_j \in  \{c_1, \ldots, c_c\}$. Every class has a set of attributes
$Att(c) = \{c.a_1, \ldots, c.a_a \}$, each of which has a domain $Dom(c.a)$ of possible values.
Upon instantiation of an object class, its attributes are given a state $o.state$
(an assignment of values to its attributes).  The underlying MDP state is the set
of all the object states: $s \in {\cal S} = \cup_{i = 1}^o \{o_i.state\}$. 

There are two advantages to using an object-oriented factored state
representation instead of a single feature vector. First, different
states in the same state space may contain different numbers of
objects of varying classes, which is useful in domains like Minecraft
in which the agent can dynamically add and remove blocks to the
world. Second, MDP states can be defined invariantly to the specific
object references.  For instance, consider a Minecraft world with two
block objects, $b_1$ and $b_2$.  If the agent picked up and swapped
the position of $b_1$ and $b_2$, the MDP state before the swap and
after the swap would be the same, because the MDP state definition is
invariant to which object holds which object state. 
This object reference invariance results in a smaller state space compared
to representations like feature vectors in which changes to value
assignments always result in a different state.

While the OO-MDP state definition is a good fit for the Minecraft
domain, our motivation for using an OO-MDP lies in the ability to
formulate predicates over classes of objects. That is, the OO-MDP
definition also includes a set of predicates ${\cal P}$ that operate
on the state of objects to provide additional high-level information
about the MDP state. 

While an OO-MDP reduces the size of the Minecraft state space
by a significant factor, the resulting state space is still far too large to
solve with any existing (OO)-MDP solver. This is the primary motivator
for incorporating affordances - to reduce the amount of the
state space that an OO-MDP agent will have to explore.

The Brown UMBC Reinforcement Learning And Planning framework (BURLAP\footnote{http://burlap.cs.brown.edu/})
is working toward integrating planning and reinforcement learning algorithms with a variety of planning domains, represented
as an OO-MDP, including ROS. In this way, transferable knowledge like affordances can be quickly deployed
to domains like Mountain Car \cite{Moore90efficientmemory-based} and Minecraft, but also to a variety
of Robots that utilize ROS. Our group is also working to deploy affordances as a
means of knowledge representation and reasoning for collaborative cooking with ReThink's Baxter.



% ====== Section: Related Work ======
\section{Related Work}
\label{sec:related-work}

% ====== Section: Affordances ======
\section{Affordances}
\label{sec:affordances}
\subsection{Learning Affordances}

% ====== Section: Experiments ======
\section{Experiments}
\label{sec:experiments}

% ==== Section: Results ====
\section{Results}
\label{sec:results}

% ====== Section: Conclusion ======
\section{Conclusion}
\label{sec:conclusion}

{\small
\bibliographystyle{plainnat}
\bibliography{main}
}
\end{document}


